---
title: "Jordan Test"
output: html_document
date: "2023-04-20"
---

```{r echo=FALSE, message=FALSE}
source(here::here("scripts/setup.R"))
```

# Modelling
Creating a new df for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features
```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))

recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df) %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  mutate(across(all_of(contains("bin")), as.factor)) %>% 
  select(rating_bin, all_of(nutritional_values), contains("bin"), contains("total"))
```

Creating training and test set. We chose a 75/25 split
```{r}
set.seed(12)
index <- createDataPartition(recipes_analysis$rating_bin, p=0.75, list = FALSE)#data partition attemps to already balance out the data based on the outcome --> but here doesn't manage fully
train_bin <- recipes_analysis[index, ]
test_bin <- recipes_analysis[-index, ]
```

However, we can see that the data is pretty unbalanced between the 2 rating classes in the training set.
```{r}
table(train_bin$rating_bin)
```

Balancing the training set
```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_bin %>%  filter(rating_bin == "good")
tr_bad <- train_bin %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))

#checking that we have the correct number of good and bad
table(upsamp_tr_bin$rating_bin)
```


## KNN - rating_bin with NUT, BIN and TOTAL - NORMALISED

```{r}
train_temp <- train_bin %>% #using unbalanced training set here as apparently it's not required for KNN for data to be balanced
  select(everything()) %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns

test_temp <- test_bin %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns
```

### CV and tuning K

```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_cv <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(115, 125,by = 2)) #initial range from 1 to 150, found 121 as best value so we reduced the range to reduce computation time
                )
knn_cv
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_bin %>% 
  mutate(predicted_prob = predict(knn_cv, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 121

We see that the relatively high accuracy is achieved by classifying most obs as good again. This is even more visible than with the non-normalised model, resulting in a worst model performance (even higher p-value to NIR model)
```{r}
knn_cv_pred <- predict(knn_cv, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_cv_pred, positive="good")
```


###########
########
###########

## KNN - rating_bin with only NUT

We normalise straight away as this is what is recommended for KNN.
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values)) %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns

test_temp <- test_bin %>%
  select(rating_bin, all_of(nutritional_values)) %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns
```

### CV and tuning K
Best K if we look at both accuracy and Kappa is 39.
```{r}
#tuning k
trCtrl <- trainControl(method = "cv",
                       summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_cv <- train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(1,51,by = 2))
                )
knn_cv
```

#### Roc curve
We can see that the model perfectly predicts all observations in the training set, which is a sign of serious overfitting. But this also confirms that 0.5 is a good cutoff point.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_cv, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 39

Once again most obs are classified as good. Not better than NIR.

```{r}
knn_cv_pred <- predict(knn_cv, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_cv_pred, positive="good")
```

## KNN - rating_bin with NUT and total
Using train_bin and not the upsampled training set, as it gave worst results for models above.
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total"))) %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns

test_temp <- test_bin %>%
  select(rating_bin, all_of(nutritional_values), all_of(contains("total"))) %>% 
  mutate(across(where(is.numeric), my_normalise))#normalising the numerical columns
```

### CV and tuning K
Best K if we look at both accuracy and Kappa is 71.
```{r}
#tuning k
trCtrl <- trainControl(method = "cv",
                       summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_cv <- train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(61,81,by = 2))#best was 71 across whole range
                )
knn_cv
```

#### Roc curve
We can see that the model perfectly predicts all observations in the training set, which is a sign of serious overfitting. But this also confirms that 0.5 is a good cutoff point.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_cv, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 71
Once again most obs are classified as good. Not better than NIR.

```{r}
knn_cv_pred <- predict(knn_cv, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_cv_pred, positive="good")
```
