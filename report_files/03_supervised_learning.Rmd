# Supervised Learning

## 0. Data Normalisation, splitting and balancing
Creating a new dataframe for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features. We name it `recipes_analysis`.

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))
  
###### CAREFUL --> recipes_analysis should be of dim 10163 x 33
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(across(all_of(contains("bin")), as.factor), ID = as.character(ID)) %>% 
  select(rating, all_of(nutritional_values), contains("bin"), contains("total"))
```

Creating a normalized version of `recipes_analysis` where the continuous numerical values are normalised, to remain consistent across all models for our analysis.

```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

recipes_analysis_normd <- recipes_analysis %>% 
  mutate(rating = as.factor(rating), across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split

```{r}
#creating training and test sets
set.seed(12)
index <- createDataPartition(recipes_analysis_normd$rating, p=0.75, list = FALSE)
```

We create a training set with the original `rating` variable with 7 levels. 

Below we can see the multilevel rating training set is really unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_multi <- recipes_analysis_normd[index, ]
test_multi <- recipes_analysis_normd[-index, ]

table(train_multi$rating)
```


### Creating train and test sets for some variable combination
```{r}
#for train multi
tr_multi_all <- train_multi
te_multi_all <- test_multi

tr_multi_Nut <- train_multi %>% 
  select(rating, all_of(nutritional_values))
te_multi_Nut <- test_multi %>% 
  select(rating, all_of(nutritional_values))
```

### TrainControl Functions
Here we create the train control functions we will use throughout the project.
```{r}
#creating train control data for unbalanced data
trCtrl <- trainControl(method = "cv",
                       number = 5)
```

```{r}
#train control with downsampling
trCtrl_down <- trainControl(method = "cv",
                       number = 5,
                       sampling = "down")
```

```{r}
#train control with downsampling twoclass summary
trCtrl_down_twoClass <- trainControl(method = "cv",
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE,
                       number = 5,
                       sampling = "down"
                       )
```

## 1.0 Testing performance on 7-level rating classification with RF

Given our correlation results in EDA, we anticipated that a classification with 7 levels will basically be equivalent to randomly classifying each observation in one of the 7 rating levels. However, to make sure this is the case, we try running a random forest on the nutritional values, as well as both the "total" and "bin" ingredients-related features we created during the EDA.
We use a random forest for this testing phase because, being an ensemble method, it should give us reasonable results if such results are possible with the data we have. If the results are bad, we can assume that other models will not magically classify everything perfectly.

### Fitting RF - multi_all

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 45.6
- Kappa 0.002
```{r}
set.seed(12)
rf_multi_all_fit <- caret::train(rating ~ .,
                     data = tr_multi_all,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_all_fit
plot(rf_multi_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions RF - multi_all
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r}
#make predictions
rf_multi_all_pred <- predict(rf_multi_all_fit, newdata = test_multi)

#confusion matrix 
confusionMatrix(data = rf_multi_all_pred, reference = test_multi$rating)
```

### Fitting - RF multi_Nut
We now try another RF but only with the nutritional values as explanatory variables to see if it changes something.

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 41.6
- Kappa 0.048

```{r}
set.seed(12)
rf_multi_Nut_fit <- train(rating ~ .,
                     data = tr_multi_Nut,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_Nut_fit
plot(rf_multi_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions - RF multi_Nut

As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r}
#make predictions
rf_multi_Nut_pred <- predict(rf_multi_Nut_fit, newdata = te_multi_all)

#confusion matrix 
confusionMatrix(data = rf_multi_Nut_pred, reference = te_multi_Nut$rating)
```

### Evaluation and decision for rest of analysis
Since we see a huge bias towards the majority class in unbalanced data (which we expected), we have to take a decision between 3 options:

- We could try balancing the 7 levels, either up or down. Due to the very large disparity in observation counts per level (min at 32 and max at 3481), we believe than neither up- nor downsampling would be ideal. Indeed, upsampling would mean creating a massive amount of artificial observations through replacement for the minority classes, while downsampling would leave us with an insufficient total amount of observations.
- We could try to aggregate the minority classes into one single class (i.e., putting  1.25, 1.875, 2.5, and 3.125 together), leaving us with 4 final classes. There would still be a significant imbalance however, and given the near-random classification that we observed above, we do not believe aggregating those classes is the solution.
- We could transform the 7 levels into a binary classification problem. This would help both with class imbalance and would hopefully also lead to higher accuracy metrics by simplifying the classification task.

After careful reflection, we believe that the 3rd options is our best bet. We therefore transform the 7-level rating variable into a binary rating variable, with the threshold at 4. This means that all ratings below 4 are now considered as "bad" and all ratings above 4 are considered as "good".

## 1.1 33 Engineered variables - unbalanced

### Methodology

### Data preparation for analysis

#### Creating new rating_bin variable
and one with a engineered rating variable which we converted to binary "bad" or "good" (i.e., with a threshold at 4 to decide if the rating is bad or good).

```{r}
#creating binary rating variable
recipes_analysis_bin <- recipes_analysis %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  select(-rating)

#reversing rating_bin factor order
recipes_analysis_bin$rating_bin <- factor(recipes_analysis_bin$rating_bin, levels=rev(levels(recipes_analysis_bin$rating_bin )))

#normalising newly created df
recipes_analysis_bin <- recipes_analysis_bin %>% 
  mutate(across(where(is.numeric), my_normalise))
```

#### Creating rating_bin training and test sets
```{r}
#creating training and test sets
set.seed(12)
index_bin <- createDataPartition(recipes_analysis_bin$rating_bin, p=0.75, list = FALSE)
```

We create a training set with the new `rating_bin` variable with only 2 levels. 

Below we can see the binary rating training set is still slightly unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_bin <- recipes_analysis_bin[index_bin, ]
test_bin <- recipes_analysis_bin[-index_bin, ]

table(train_bin$rating_bin)
```
#### Creating various train/test sets with different variable combinations
```{r}
#creating train and test set for all combinations of variables, for binary rating df
train_bin_Nut <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))
test_bin_Nut <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))

train_bin_NuTo <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
test_bin_NuTo <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

train_bin_NuBi <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_bin_NuBi <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))

train_bin_Tot <- train_bin %>% 
  select(rating_bin, all_of(contains("total")))
test_bin_Tot <- test_bin %>% 
  select(rating_bin, all_of(contains("total")))

train_bin_Bin <- train_bin %>% 
  select(rating_bin, all_of(contains("bin")))
test_bin_Bin <- test_bin %>% 
  select(rating_bin, all_of(contains("bin")))
```

### Logistic Regression - unbalanced - Nut

```{r}
# 
# # to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# # unbalanced data
# # 
# train_bin_01 <- train_bin %>%
#   mutate(rating_bin = ifelse(rating_bin=="good",1,0))
# test_bin_01 <- test_bin %>%
#   mutate(rating = ifelse(rating_bin=="good",1,0))

###########
#GIVES THE SAME RESULTS WITH GOOD/BAD and 0/1 as rating_bin, be it as factors or not
```

```{r}

# since the data has already been split we proceed with the fitting of the logistic regression
# nutritional values

rating_logr <- glm(rating_bin ~., data=train_bin_Nut, family="binomial")

summary(rating_logr)

```

#### Variable selection and interpretation

```{r}
rating_logr_sel <- step(rating_logr)
summary(rating_logr_sel)
```

TO REVIEW TEXT:

The variables calories, total_fruits, total_carbs, total_fruits and seafood_bin are statistically significant at alpha=0.01. For instance, we can observe that the probability of having a "good" rating increases with a higher value of calories per recipe, but it decreases with a larger number of food high in carbs per recipe.

The variables total_fish, total_herbs, total_cheese and meats_bin are statistically significant at alpha=0.05. In this case we would see the probability of having a "good" rating increasing with a higher number of cheeses per recipe.

#### Inference

```{r}
prob_te_rating <- predict(rating_logr_sel, newdata = test_bin_Nut, type="response")
pred_te_rating <- as.factor(ifelse(prob_te_rating >= 0.5, "good", "bad"))
table(pred_te_rating)

confusionMatrix(reference = test_bin_Nut$rating_bin, data = prob_te_rating, positive="good")

# table(Pred=pred_te_rating, Obs=test_bin_Nut$rating_bin)
# 
# accuracy_rating <- sum(pred_te_rating == test_bin_Nut$rating_bin) / length(test_bin_Nut$rating_bin)
# 
# print(paste("Accuracy:", accuracy_rating))
```

As we can see the predictions are not satisfying. Since the number of good recipes is larger than the number of bad recipes, predicting a 1 will always provide a good model overall. Anyway the model is not reliable when it comes to predict recipes with a 0.

We believe that it is worth to proceed with the same analysis using balanced data.

### SVM - unbalanced - Nut

Only nutritional values

```{r}

recipe_svm_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium , data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm_nutritional

```

Let us make predictions and check the accuracy

```{r}

recipe_svm_pred_nutritional <- predict(recipe_svm_nutritional, newdata = test_bin)

table(Pred=recipe_svm_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred_nutritional, reference = test_bin$rating_bin)

```

In this case the accuracy stands at 59.1%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the accuracy.

### SVM - unbalanced - NuBi

```{r}
# with the svm() function of the e1071 package we can fit SVM to the data with several possible kernels 
# here we start with the linear kernel

# unbalanced data

train_bin$rating_bin <- as.factor(train_bin$rating_bin)
test_bin$rating_bin <- as.factor(test_bin$rating_bin)


recipe_svm <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm

```

Let us make predictions and check the accuracy

```{r}

recipe_svm_pred <- predict(recipe_svm, newdata = test_bin)

table(Pred=recipe_svm_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred, reference = test_bin$rating_bin)

```

In this case the accuracy stands at 59%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the quality of our analysis.

### KNN - unbalanced - Nut

#### Fitting
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```


Best K if we look at both accuracy and Kappa is 111.

```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_Nu <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_pred <- predict(knn_Nu, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_pred, positive="good")
```

### KNN - unbalanced - NuBi

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

#### Fitting
Best K if we look at both accuracy and Kappa is 135

```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi
```

#### Predictions

Once again most obs are classified as good. Not better than NIR.

```{r}
knn_Bi_pred <- predict(knn_NuBi, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_pred, positive="good")
```

### CART - unbalanced - Nut

```{r}
#using all features
train_temp <- train_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
  select(all_of(nutritional_values), contains("bin"))
```

#### Plotting initial tree

```{r}
cart_all <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_all_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_pruned, roundint=FALSE)
```

#### Making predictions

```{r eval=FALSE}
#predicting the test set
cart_all_pruned_pred <- predict(cart_all_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_all_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

### CART - unbalanced - NuBi

```{r}
#using all features
train_temp <- train_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
  select(all_of(nutritional_values), contains("bin"))
```

#### Plotting initial tree

```{r}
cart_NuBi <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_NuBi_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_pruned, roundint=FALSE)
```

#### Making predictions

```{r eval=FALSE}
#predicting the test set
cart_NuBi_pruned_pred <- predict(cart_NuBi_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_NuBi_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

## 1.2. 33 Engineered variables - balanced

```{r}
#function to create the summary tables from the confusion matrices
summary_table_fun <- function(x){
  acc <- x[[3]][[1]]
  bacc <- x[[4]][[11]]
  spec <- x[[4]][[2]]
  vec <- c(acc, bacc, spec)
  return(vec)
}

#creating empty summary table for balanced data
summary_table_down <- tibble(metric = c("accuracy", "balanced_accuracy", "specificity"))
```

### Random Forest - balanced - Nut

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 53.8
- BACC 53.8
- Kappa 7.40
- Recall 53.9

#### Fitting
```{r rf_nut_fit}
set.seed(12)
rf_Nut_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Nut,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Nut_fit
plot(rf_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r rf_nut_pred}
set.seed(12)
#make predictions
rf_Nut_pred <- predict(rf_Nut_fit, newdata = test_bin_Nut)

#confusion matrix 
CM <- confusionMatrix(data = rf_Nut_pred, reference = test_bin_Nut$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_Nut = summary_table_fun(CM))
```

### Random Forest - balanced - NuBi

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- ACC 54.8
- BACC 54.2
- Kappa 8.20
- Recall 57.7

#### Fitting
```{r rf_nubi_fit}
set.seed(12)
rf_NuBi_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuBi,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuBi_fit
plot(rf_NuBi_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_nubi_pred}
set.seed(12)
#make predictions
rf_NuBi_pred <- predict(rf_NuBi_fit, newdata = test_bin_NuBi)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuBi_pred, reference = test_bin_NuBi$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuBi = summary_table_fun(CM))
```

### Random Forest - balanced - NuTo

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 56.4
- BACC 55.8
- Kappa 11.3
- Recall 59.3

#### Fitting
```{r rf_nuto_fit}
set.seed(12)
rf_NuTo_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuTo,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuTo_fit
plot(rf_NuTo_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_nuto_pred}
set.seed(12)
#make predictions
rf_NuTo_pred <- predict(rf_NuTo_fit, newdata = test_bin_NuTo)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuTo_pred, reference = test_bin_NuTo$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuTo = summary_table_fun(CM))
```

### Random Forest - balanced - Tot

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- ACC 52.5
- BACC 52.4
- Kappa 4.6
- Recall 52.9

#### Fitting
```{r rf_tot_fit}
set.seed(12)
rf_Tot_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Tot,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Tot_fit
plot(rf_Tot_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_tot_pred}
set.seed(12)
#make predictions
rf_Tot_pred <- predict(rf_Tot_fit, newdata = test_bin_Tot)

#confusion matrix 
CM <- confusionMatrix(data = rf_Tot_pred, reference = test_bin_Tot$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Tot = summary_table_fun(CM))
```

### Random Forest - balanced - Bin

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 52.9
- BACC 52.4
- Kappa 4.6
- Sens 55.4

#### Fitting
```{r rf_bin_fit}
set.seed(12)
rf_Bin_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Bin,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Bin_fit
plot(rf_Bin_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_bin_pred}
set.seed(12)
#make predictions
rf_Bin_pred <- predict(rf_Bin_fit, newdata = test_bin_Bin)

#confusion matrix 
CM <- confusionMatrix(data = rf_Bin_pred, reference = test_bin_Bin$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Bin = summary_table_fun(CM))
```
We see that the 3 best performing models are: Nut, NuBi and NuTo. Which was expected given the importance of the nutritional values that we saw in the PCA during the EDA. Therefore, we decide to test further models only with those 3 combinations of variables, thus leaving out models trained solely on Tot or on Bin.

### Logistic Regression - balanced - Nut

Focus on nutritional values only

```{r}

rating_logr_up_nutritional <- glm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional <- step(rating_logr_up_nutritional)
summary(rating_logr_sel_up_nutritional)

```

We can observe only two variables which are statistically significant at alpha=0.05. More in particular, we can observe that the probability of having a "good" rating increases with a higher content of fat and sodium per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional <- predict(rating_logr_sel_up_nutritional, newdata = test_bin, type="response")
pred_te_rating_up_nutritional <- ifelse(prob_te_rating_up_nutritional >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional <- sum(pred_te_rating_up_nutritional == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional))

```

In this case the accuracy is lower compared to the model with all variables included.

### Logistic Regression - balanced - NuTo

Focus on nutritional values and total

```{r}

rating_logr_up_nutritional_total <- glm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_vegetables + total_meat + total_fish + total_seafood + total_herbs + total_nuts + total_fruits + total_nuts + total_fruits + total_cheese + total_dairy + total_spices + total_cereals + total_carbs + total_dessert, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_total)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional_total <- step(rating_logr_up_nutritional_total)
summary(rating_logr_sel_up_nutritional_total)

```

Among the variables selected, calories, total_carbs, total_fruits, total_meat, total_seafood are statistically significant at alpha=0.01. The rest of the variables are statistically significant at alpha=0.05. For instance, we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional_total <- predict(rating_logr_sel_up_nutritional_total, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_total <- ifelse(prob_te_rating_up_nutritional_total >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_total, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_total <- sum(pred_te_rating_up_nutritional_total == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_total))

```

Better to add "total" variables to the model rather than using only nutritional values. The accuracy increases from 47.9% to 53.1%.

### Logistic Regression - balanced - NuBi

Focus on nutritional values and bins

```{r}

rating_logr_up_nutritional_bin <- glm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_bin)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional_bin <- step(rating_logr_up_nutritional_bin)
summary(rating_logr_sel_up_nutritional_bin)

```

Among the variables selected, calories, seafood_bin, fruits_bin, carbs_bin, meats_bin, fish_bin and nuts_bin are statistically significant at alpha=0.01. The rest of the variables, apart from cheese_bin, are statistically significant at alpha=0.05. For instance, also in this case we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional_bin <- predict(rating_logr_sel_up_nutritional_bin, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_bin <- ifelse(prob_te_rating_up_nutritional_bin >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_bin, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_bin <- sum(pred_te_rating_up_nutritional_bin == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_bin))

```

Better to add "bin" variables to the model rather than using nutritional values and total. The accuracy slightly improves from 53.1% to 53.7%.

### SVM - balanced - Nut
#### Linear SVM - balanced - Nutri

```{r}
upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up_nutritional
```

```{r}

recipe_svm_up_pred_nutritional <- predict(recipe_svm_up_nutritional, newdata = test_bin)

table(Pred=recipe_svm_up_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred_nutritional, reference = test_bin$rating_bin)

```

Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 43%. We will proceed the rest of the analysis with balanced data.

##### Tuning the Hyperparameters

```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear_nutritional

```

The validation accuracy stands at 50.9%, which is not that high even though it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_nutritional

plot(svm_Linear_Grid_nutritional)

```

```{r}

svm_Linear_Grid_nutritional$bestTune

```

The result indicates that setting the cost to C=10 provides the best model with accuracy=50.7%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.

#### Radial SVM - balanced - Nutri

```{r}

recipe_rb_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb_nutritional

```

Let us make predictions and check the accuracy

```{r}

recipe_rb_pred_nutritional <- predict(recipe_rb_nutritional, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred_nutritional, reference = test_bin$rating_bin)

```

The accuracy is now 52.5% meaning that the radial basis kernel seems to do much better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.


##### Tuning the Hyperparameters - Radial basis SVM

```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid_nutritional

plot(svm_Radial_Grid_nutritional)

```

```{r}

svm_Radial_Grid_nutritional$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=55.7%.

#### SVM Nutri - Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid_nutritional$bestTune$sigma, 
                       cost = svm_Radial_Grid_nutritional$bestTune$C)

recipe_rb_tuned_pred_nutritional <- predict(recipe_rb_tuned_nutritional, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred_nutritional, reference = test_bin$rating_bin)

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52.8% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1000 and sigma=0.01 is the best model.
### SVM - balanced - NuBi
#### Linear SVM - balanced - NuBi

```{r}

upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up

```

```{r}

recipe_svm_up_pred <- predict(recipe_svm_up, newdata = test_bin)

table(Pred=recipe_svm_up_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred, reference = test_bin$rating_bin)

```

Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 57.3%. We will proceed the rest of the analysis with balanced data.

##### Tuning the Hyperparameters

```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear

```

The validation accuracy stands at 53.2%, which is not that high even though it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

```

```{r}

svm_Linear_Grid$bestTune

```

The result indicates that setting the cost to C=1000 provides the best model with accuracy=54.5%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.


#### Radial SVM - balanced - NuBi

```{r}

recipe_rb <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb

```

Let us make predictions and check the accuracy

```{r}

recipe_rb_pred <- predict(recipe_rb, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred, reference = test_bin$rating_bin)

```

The accuracy is now 53.3% meaning that the radial basis kernel seems to do slightly better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.

##### Tuning the Hyperparameters 

```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid

plot(svm_Radial_Grid)

```

```{r}

svm_Radial_Grid$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=64.2%.

#### SVM NuBi - Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid$bestTune$sigma, 
                       cost = svm_Radial_Grid$bestTune$C)

recipe_rb_tuned_pred <- predict(recipe_rb_tuned, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred, reference = test_bin$rating_bin)

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 54.3% on the test set. We can conclude that among the models , the linear kernal SVM without tuned hyperparameters represents the best model with 57.3% accuracy.

### KNN - balanced - Nut

#### Fitting

```{r knn_nut_fit}
set.seed(12)
knn_Nut <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))#initially checked on 1-151 range, best was 111
                )
knn_Nut
```

#### Predictions

```{r knn_nut_pred}
knn_Nut_pred <- predict(knn_Nut, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = knn_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, KNN_Nut = summary_table_fun(CM))
```

### KNN - balanced - NuBi

#### Fitting

```{r knn_nubi_fit}
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))#initially checked on 1-151 range, best was 37
                )
knn_NuBi
```

#### Predictions

```{r knn_nubi_pred}
knn_NuBi_pred <- predict(knn_NuBi, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = knn_NuBi_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuBi = summary_table_fun(CM))
```

### KNN - balanced - NuTo

#### Fitting

```{r knn_nuto_fit}
set.seed(12)
knn_NuTo <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))
                )
knn_NuTo
```

#### Predictions

```{r knn_nuto_pred}
knn_NuTo_pred <- predict(knn_NuTo, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = knn_NuTo_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuTo = summary_table_fun(CM))
```

### CART - balanced -  Nut

**Hyperparameters**

Best parameter CP is 0.0035

**Accuracy Metrics**

- ACC 54.80

#### Fitting
```{r cart_nut_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_Nut_fit <- train(rating_bin ~ .,
                  data = train_bin_Nut,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_Nut_fit
```
It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nut_results}
# Get the results for each CP step
cart_Nut_results <- cart_Nut_fit$results

cart_Nut_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nut_pred}
#Now using the cv to predict test set
cart_Nut_pred <- predict(cart_Nut_fit, newdata = test_bin_Nut)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_Nut = summary_table_fun(CM))
```

### CART - balanced - NuBi

**Hyperparameters**

Best parameter CP is 0.00546

**Accuracy Metrics**

- ACC 53.7

#### Fitting
```{r cart_nubi_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuBi_fit <- train(rating_bin ~ .,
                  data = train_bin_NuBi,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuBi_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nubi_results}
# Get the results for each CP step
cart_NuBi_results <- cart_NuBi_fit$results

cart_NuBi_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nubi_pred}
#Now using the cv to predict test set
cart_NuBi_pred <- predict(cart_NuBi_fit, newdata = test_bin_NuBi)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuBi_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuBi = summary_table_fun(CM))
```

### CART - balanced - NuTo

**Hyperparameters**

Best parameter CP is 0.00257

**Accuracy Metrics**

- ACC 53.7

#### Fitting
```{r cart_nuto_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuTo_fit <- train(rating_bin ~ .,
                  data = train_bin_NuTo,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuTo_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nuto_results}
# Get the results for each CP step
cart_NuTo_results <- cart_NuTo_fit$results

cart_NuTo_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nuto_pred}
#Now using the cv to predict test set
cart_NuTo_pred <- predict(cart_NuTo_fit, newdata = test_bin_NuTo)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuTo_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuTo = summary_table_fun(CM))
```


## 2.1 All variables (Nut + 293 ingredients)

### Creating dataset
```{r}
analysis_all <- recipes %>% 
  select(rating, all_of(c(nutritional_values, all_ingredients))) %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good")), across(all_of(all_ingredients), as.factor)) %>% 
  select(-rating) %>% 
  select(rating_bin, everything())

#reversing rating_bin factor order
analysis_all$rating_bin <- factor(analysis_all$rating_bin, levels=rev(levels(analysis_all$rating_bin)))

#normalising newly created df
analysis_all <- analysis_all %>% 
  mutate(across(where(is.numeric), my_normalise))
```

```{r}
#we noticed that 5 ingredients appear in none of the recipes
ingredients_to_remove <- analysis_all %>%
  select_if(~nlevels(.) == 1) %>% colnames()

#removing those 5 ingredients
analysis_all <- analysis_all %>% 
  select(-all_of(ingredients_to_remove))
```

```{r}
#creating training and test sets
set.seed(12)
index_all <- createDataPartition(analysis_all$rating_bin, p=0.75, list = FALSE)
```

We create a training set with the new `rating_bin` variable with only 2 levels, including the nutritional and all ingredients dummies, instead of the engineered variables used in the sub-section above.

```{r}
#creating training and test set with multilevel rating variable
train_all <- analysis_all[index_all, ]
test_all <- analysis_all[-index_all, ]

table(train_all$rating_bin)
```
#### Balancing the binary training set through manual downsampling

```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_all %>%  filter(rating_bin == "good")
tr_bad <- train_all %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_good <- sample(x = 1:nrow(tr_good), size = nrow(tr_bad), replace = FALSE)
downsamp_tr_all <- tibble(rbind(tr_good[index_good,], tr_bad))

#checking that we have the correct number of good and bad
table(downsamp_tr_all$rating_bin)
```

```{r}
#manual upsampling

# #filtering by rating class
# set.seed(12)
# tr_good <- train_bin %>%  filter(rating_bin == "good")
# tr_bad <- train_bin %>%  filter(rating_bin == "bad")
# 
# #indexing "bad" and creating new resampled training set
# index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
# upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))
# 
# #checking that we have the correct number of good and bad
# table(upsamp_tr_bin$rating_bin)
```

### Random Forest - balanced - All Var

**Hyperparameters**

Best tune: mtry = 292

**Accuracy metrics**
With tunelength = 2
- ACC 56.7
- BACC 56.8
- Kappa 13.1
- Recall 56.2

#### Fitting
```{r rf_all_fit}
set.seed(12)
rf_all_fit <- train(rating_bin ~ .,
                    data = train_all,
                    method = "rf",
                    trControl = trCtrl_down,
                    tuneLength = 3)
#show the random forest with cv 
rf_all_fit
plot(rf_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_all_pred}
set.seed(12)
#make predictions
rf_all_pred <- predict(rf_all_fit, newdata = test_all)

#confusion matrix 
confusionMatrix(data = rf_all_pred, reference = test_all$rating_bin)
```

### XGBoost - Unbalanced
```{r xgb_all datasets}
numeric_tr <- train_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

numeric_downsamp_tr <- downsamp_tr_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

numeric_te <- test_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

#creating datasets which don't include the labels, for inputs in the xgb Matrix
explan_tr <- numeric_tr %>% 
  select(-rating_bin)

explan_downsamp_tr <- numeric_downsamp_tr %>% 
  select(-rating_bin)

explan_te <- numeric_te %>% 
  select(-rating_bin)

#prepare data for XGBoost by creating xgb matrix
xgb_tr <- xgb.DMatrix(data = as.matrix(explan_tr), label = numeric_tr$rating_bin)
xgb_downsamp_tr <- xgb.DMatrix(data = as.matrix(explan_downsamp_tr), label = numeric_downsamp_tr$rating_bin)
xgb_te <- xgb.DMatrix(data = as.matrix(explan_te), label = numeric_te$rating_bin)
```

**Accuracy metrics**

- ACC 59.4
- BACC 55.7
- Kappa 11.9
- Recall 76.4

#### Fitting
```{r xgb_all_unbal_fit}
#set hyperparameters
param_list <- list(
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 5,
  min_child_weight = 1,
  subsample = 0.6,
  colsample_bytree = 0.6
)

#train the model
set.seed(12)
xgb_unbal_fit <- xgboost(data = xgb_tr,
                     params = param_list,
                     nrounds = 500,
                     verbose = FALSE)
```

#### Predictions
```{r xgb_all_unbal_pred}
xgb_unbal_prob <- predict(xgb_unbal_fit, newdata = xgb_te)

xgb_unbal_pred <- as.factor(ifelse(xgb_unbal_prob<0.5, 0, 1))

confusionMatrix(reference = as.factor(numeric_te$rating_bin), data = xgb_unbal_pred, positive = "1")
```

Only marginally improves accuracy, but greatly improves balanced accuracy and Kappa.
```{r xgb_all_unbal threshold}
rocit_emp <- rocit(score = xgb_unbal_prob, class = numeric_te$rating_bin, method = "emp")
plot(rocit_emp, col = c(1,"gray50"),legend = TRUE, YIndex = TRUE)

#best value is 0.5501

xgb_unbal_pred_threshold <- as.factor(ifelse(xgb_unbal_prob<0.5501, 0, 1))

confusionMatrix(reference = as.factor(numeric_te$rating_bin), data = xgb_unbal_pred_threshold, positive = "1")
```
### XGBoost - Balanced

**Accuracy metrics**

- ACC 59.4
- BACC 55.7
- Kappa 11.9
- Recall 76.4

#### Fitting
```{r xgb_all_down_fit}
#set hyperparameters
param_list <- list(
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 5,
  min_child_weight = 1,
  subsample = 0.6,
  colsample_bytree = 0.6
)

#train the model
set.seed(12)
xgb_down_fit <- xgboost(data = xgb_downsamp_tr,
                     params = param_list,
                     nrounds = 500,
                     verbose = FALSE)
```

#### Predictions
```{r xgb_all_down_pred}
xgb_down_prob <- predict(xgb_down_fit, newdata = xgb_te)

xgb_down_pred <- as.factor(ifelse(xgb_down_prob<0.5, 0, 1))

confusionMatrix(reference = as.factor(numeric_te$rating_bin), data = xgb_down_pred, positive = "1")
```

### Logistic Regression
```{r logreg_all_fit}
#too long
logreg_all_fit <- train(rating_bin~., 
                       data=downsamp_tr_all, 
                       method="glmStepAIC", 
                       family = "binomial",
                       trControl=trCtrl_down,
                       trace = TRUE)
logreg_all_fit
```
### Predictions
```{r logreg_all_pred}
#predicting
logreg_all_pred <- predict(logreg_all_fit, newdata = test_all)
#creating confusion matrix
CM <- confusionMatrix(reference = te$response, data = logreg_all_pred, positive="1")
CM
#adding results to the summary_table_unbal
summary_table_all <- cbind(summary_table_unbal, LogReg_all = summary_table_fun(CM))
```

### CART

**Hyperparameters**

Best parameter CP is 0.0035

**Accuracy Metrics**

- ACC 54.80

#### Fitting
```{r cart_all_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_all_fit <- train(rating_bin ~ .,
                  data = downsamp_tr_all,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_all_fit
```

```{r cart_all_results}
# Get the results for each CP step
cart_all_results <- cart_all_fit$results

cart_all_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_all_pred}
#Now using the cv to predict test set
cart_all_pred <- predict(cart_all_fit, newdata = test_all)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_all_pred)
CM

summary_table_all <- cbind(summary_table_down, CART_all = summary_table_fun(CM))
```

## 3.1 Variable Importance
```{r VarImp XGBoost}
#XGBoost variable importance
xgb_importance_matrix <- xgb.importance(model = xgb_down_fit)
xgb_importance_matrix

xgb.ggplot.importance(importance_matrix = xgb_importance_matrix) +
  labs(title = "XGBoost Feature Importance")
```

