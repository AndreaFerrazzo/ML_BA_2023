# Supervised Learning

Creating a new df for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features
```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))

###### CAREFUL --> recipes_analysis should be of dim 10163 x 34
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  mutate(across(all_of(contains("bin")), as.factor)) %>% 
  mutate(ID = as.character(ID)) %>% 
  select(rating_bin, all_of(nutritional_values), contains("bin"), contains("total"))
```

Normalising the continuous numerical values, to remain consistent across all models.
```{r}
recipes_analysis <- recipes_analysis %>% 
  mutate(across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split
```{r}
set.seed(12)
index <- createDataPartition(recipes_analysis$rating_bin, p=0.75, list = FALSE)#data partition attemps to already balance out the data based on the outcome --> but here doesn't manage fully

train_bin <- recipes_analysis[index, ]
test_bin <- recipes_analysis[-index, ]
```

However, we can see that the data is pretty unbalanced between the 2 rating classes in the training set.
```{r}
table(train_bin$rating_bin)
```

Balancing the training set through upsampling
```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_bin %>%  filter(rating_bin == "good")
tr_bad <- train_bin %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))

#checking that we have the correct number of good and bad
table(upsamp_tr_bin$rating_bin)
```
# Analysis Unbalanced outcome variable

## Logistic Regression

```{r}

# to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# unbalanced data

train_bin$rating_bin <- ifelse(train_bin$rating_bin=="good",1,0)
test_bin$rating_bin <- ifelse(test_bin$rating_bin=="good",1,0)

```

```{r}

# since the data has already been split we proceed with the fitting of the logistic regression
# nutritional values, categories and total ingredients 

rating_logr <- glm(rating_bin ~., data=train_bin, family="binomial")

summary(rating_logr)

```

### Variable selection and interpretation
```{r}

rating_logr_sel <- step(rating_logr)
summary(rating_logr_sel)

```
The variables calories, total_fruits, total_carbs, total_fruits and seafood_bin are statistically significant at alpha=0.01. For instance, we can observe that the probability of having a "good" rating increases with a higher value of calories per recipe, but it decreases with a larger number of food high in carbs per recipe.

The variables total_fish, total_herbs, total_cheese and meats_bin are statistically significant at alpha=0.05. In this case we would see the probability of having a "good" rating increasing with a higher number of cheeses per recipe.

### Inference
```{r}

prob_te_rating <- predict(rating_logr_sel, newdata = test_bin, type="response")
pred_te_rating <- ifelse(prob_te_rating >= 0.5, 1, 0)

table(Pred=pred_te_rating, Obs=test_bin$rating_bin)

accuracy_rating <- sum(pred_te_rating == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating))


```
As we can see the predictions are not satisfying. Since the number of good recipes is larger than the number of bad recipes, predicting a 1 will always provide a good model overall. Anyway the model is not reliable when it comes to predict recipes with a 0. 

We believe that it is worth to proceed with the same analysis using balanced data.

## Logistic Regression with balanced data

```{r}

# to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# the same procedure on the test_bin has already been applied before

upsamp_tr_bin$rating_bin <- ifelse(upsamp_tr_bin$rating_bin=="good",1,0)

```

```{r}

# since the data has already been splitted we proceed with the fitting of the logistic regression
# nutritional values, categories and total ingredients 

rating_logr_up <- glm(rating_bin ~., data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up)

```

### Variable selection and interpretation
```{r}

rating_logr_sel_up <- step(rating_logr_up)
summary(rating_logr_sel_up)

```
Among the variables selected, total_carbs, total_fruits, total_meat, seafood_bin and calories are statistically significant at alpha=0.01. For instance, we can observe that the probability of having a "good" rating increases with a higher number of fruits per recipe, but it decreases with a larger number of food high in carbs per recipe. Additionally, the probability of having a "good" rating increases with the content of calories.

### Inference

```{r}

prob_te_rating_up <- predict(rating_logr_sel_up, newdata = test_bin, type="response")
pred_te_rating_up <- ifelse(prob_te_rating_up >= 0.5, 1, 0)

table(Pred=pred_te_rating_up, Obs=test_bin$rating_bin)

accuracy_rating_up <- sum(pred_te_rating_up == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up))

```
Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has worsened at predicting "good" ratings. The use of balanced data has then proved to be useful.


## Logistic Regression with balanced data 
Focus on nutritional values only
```{r}

rating_logr_up_nutritional <- glm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional)

```


### Variable selection and interpretation
```{r}

rating_logr_sel_up_nutritional <- step(rating_logr_up_nutritional)
summary(rating_logr_sel_up_nutritional)

```

We can observe only two variables which are statistically significant at alpha=0.05. More in particular, we can observe that the probability of having a "good" rating increases with a higher content of fat and sodium per recipe.

### Inference
```{r}

prob_te_rating_up_nutritional <- predict(rating_logr_sel_up_nutritional, newdata = test_bin, type="response")
pred_te_rating_up_nutritional <- ifelse(prob_te_rating_up_nutritional >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional <- sum(pred_te_rating_up_nutritional == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional))

```
In this case the accuracy is lower compared to the model with all variables included.


## Logistic Regression with balanced data 
Focus on nutritional values and total
```{r}

rating_logr_up_nutritional_total <- glm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_vegetables + total_meat + total_fish + total_seafood + total_herbs + total_nuts + total_fruits + total_nuts + total_fruits + total_cheese + total_dairy + total_spices + total_cereals + total_carbs + total_dessert, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_total)

```


### Variable selection and interpretation
```{r}

rating_logr_sel_up_nutritional_total <- step(rating_logr_up_nutritional_total)
summary(rating_logr_sel_up_nutritional_total)

```

Among the variables selected, calories, total_carbs, total_fruits, total_meat, total_seafood are statistically significant at alpha=0.01. The rest of the variables are statistically significant at alpha=0.05. For instance, we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

### Inference
```{r}

prob_te_rating_up_nutritional_total <- predict(rating_logr_sel_up_nutritional_total, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_total <- ifelse(prob_te_rating_up_nutritional_total >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_total, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_total <- sum(pred_te_rating_up_nutritional_total == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_total))

```
Better to add "total" variables to the model rather than using only nutritional values. The accuracy increases from 47.9% to 53.1%.


## Logistic Regression with balanced data 
Focus on nutritional values and bins
```{r}

rating_logr_up_nutritional_bin <- glm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_bin)

```


### Variable selection and interpretation
```{r}

rating_logr_sel_up_nutritional_bin <- step(rating_logr_up_nutritional_bin)
summary(rating_logr_sel_up_nutritional_bin)

```

Among the variables selected, calories, seafood_bin, fruits_bin, carbs_bin, meats_bin, fish_bin and nuts_bin are statistically significant at alpha=0.01. The rest of the variables, apart from cheese_bin, are statistically significant at alpha=0.05. For instance, also in this case we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

### Inference
```{r}

prob_te_rating_up_nutritional_bin <- predict(rating_logr_sel_up_nutritional_bin, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_bin <- ifelse(prob_te_rating_up_nutritional_bin >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_bin, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_bin <- sum(pred_te_rating_up_nutritional_bin == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_bin))

```
Better to add "bin" variables to the model rather than using nutritional values and total. The accuracy slightly improves from 53.1% to 53.7%.


################

## SVM
Only nutritional values
```{r}

recipe_svm_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium , data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm_nutritional

```

Let us make predictions and check the accuracy
```{r}

recipe_svm_pred_nutritional <- predict(recipe_svm_nutritional, newdata = test_bin)

table(Pred=recipe_svm_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred_nutritional, reference = test_bin$rating_bin)

```
In this case the accuracy stands at 59.1%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the accuracy.


## SVM - balanced data

```{r}

upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up_nutritional

```

```{r}

recipe_svm_up_pred_nutritional <- predict(recipe_svm_up_nutritional, newdata = test_bin)

table(Pred=recipe_svm_up_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred_nutritional, reference = test_bin$rating_bin)

```
Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 43%.
We will proceed the rest of the analysis with balanced data.

### Radial basis SVM
```{r}

recipe_rb_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb_nutritional

```

Let us make predictions and check the accuracy
```{r}

recipe_rb_pred_nutritional <- predict(recipe_rb_nutritional, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred_nutritional, reference = test_bin$rating_bin)

```
The accuracy is now 52.5% meaning that the radial basis kernel seems to do much better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.


### Tuning the Hyperparameters - Linear SVM
```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear_nutritional

```
The validation accuracy stands at 50.9%, which is not that high even though it is computed on the training set.
The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid_nutritional

plot(svm_Linear_Grid_nutritional)

```


```{r}

svm_Linear_Grid_nutritional$bestTune

```
The result indicates that setting the cost to C=10 provides the best model with accuracy=50.7%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.

### Tuning the Hyperparameters - Radial basis SVM
```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid_nutritional

plot(svm_Radial_Grid_nutritional)

```


```{r}

svm_Radial_Grid_nutritional$bestTune

```
The optimal model from this search is with sigma = 0.1 and C = 1000
This optimal model would then reach accuracy=55.7%.

### Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid_nutritional$bestTune$sigma, 
                       cost = svm_Radial_Grid_nutritional$bestTune$C)

recipe_rb_tuned_pred_nutritional <- predict(recipe_rb_tuned_nutritional, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred_nutritional, reference = test_bin$rating_bin)

```
The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52.8% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1000 and sigma=0.01 is the best model.

###########

## SVM
```{r}
library(e1071)
# with the svm() function of the e1071 package we can fit SVM to the data with several possible kernels 
# here we start with the linear kernel
# unbalanced data

train_bin$rating_bin <- as.factor(train_bin$rating_bin)
test_bin$rating_bin <- as.factor(test_bin$rating_bin)


recipe_svm <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm

```

Let us make predictions and check the accuracy
```{r}

recipe_svm_pred <- predict(recipe_svm, newdata = test_bin)

table(Pred=recipe_svm_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred, reference = test_bin$rating_bin)

```
In this case the accuracy stands at 59%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the quality of our analysis.


## SVM - balanced data

```{r}

upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up

```

```{r}

recipe_svm_up_pred <- predict(recipe_svm_up, newdata = test_bin)

table(Pred=recipe_svm_up_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred, reference = test_bin$rating_bin)

```
Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 57.3%.
We will proceed the rest of the analysis with balanced data.

### Radial basis SVM
```{r}

recipe_rb <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb

```

Let us make predictions and check the accuracy
```{r}

recipe_rb_pred <- predict(recipe_rb, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred, reference = test_bin$rating_bin)

```
The accuracy is now 53.3% meaning that the radial basis kernel seems to do slightly better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.


### Tuning the Hyperparameters - Linear SVM
```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear

```
The validation accuracy stands at 53.2%, which is not that high even though it is computed on the training set.
The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

```


```{r}

svm_Linear_Grid$bestTune

```
The result indicates that setting the cost to C=1000 provides the best model with accuracy=54.5%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.

### Tuning the Hyperparameters - Radial basis SVM
```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid

plot(svm_Radial_Grid)

```


```{r}

svm_Radial_Grid$bestTune

```
The optimal model from this search is with sigma = 0.1 and C = 1000
This optimal model would then reach accuracy=64.2%.

### Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid$bestTune$sigma, 
                       cost = svm_Radial_Grid$bestTune$C)

recipe_rb_tuned_pred <- predict(recipe_rb_tuned, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred, reference = test_bin$rating_bin)

```
The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 54.3% on the test set. We can conclude that among the models , the linear kernal SVM without tuned hyperparameters represents the best model with 57.3% accuracy.

########################

## KNN

## KNN - NUT, BIN and TOTAL

```{r}
#using unbalanced training set here as apparently it's not required for KNN for data to be balanced
train_temp <- train_bin

test_temp <- test_bin
```

### Basic Model
Trying to fit a basic KNN model with arbitrary K value of 3. Results are not different than that of a No Information Rate (NIR) model.
```{r}
mod_knn <- knn3(data = train_temp, rating_bin ~., k=3)

knn_pred <- predict(mod_knn, newdata = test_temp, type = "class")

confusionMatrix(reference = test_temp$rating_bin, data = knn_pred, positive="good")
```

### Tuning K
Best K if we look at both accuracy and Kappa is 115.
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_all <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(111, 121,by = 2))#initially checked on 1-151 range, best was 115
                )
knn_all
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_all, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 115
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_all_pred <- predict(knn_all, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_all_pred, positive="good")
```

## KNN - only NUT

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 111.
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_Nu <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_Nu, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 111
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_pred <- predict(knn_Nu, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_pred, positive="good")
```

## KNN - NUT and TOTAL
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 139
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuTo <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(135, 145,by = 2))#initially checked on 1-151 range, best was 139
                )
knn_NuTo
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuTo, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 139
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_NuTo_pred <- predict(knn_NuTo, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_NuTo_pred, positive="good")
```

## KNN - NUT and BIN

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 135
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuBi, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 135
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_Bi_pred <- predict(knn_NuBi, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_pred, positive="good")
```

## CART - NUT, BIN and TOTAL
```{r}
#using all features
train_temp <- train_bin

test_temp <- test_bin
```

### Plotting initial tree
```{r}
cart_all <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all)
```
### Pruning the tree
We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_all_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_pruned, roundint=FALSE)
```

### Making predictions
```{r eval=FALSE}
#predicting the test set
cart_all_pruned_pred <- predict(cart_all_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_all_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

## CART - NUT and BIN
```{r}
#using all features
train_temp <- train_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
  select(all_of(nutritional_values), contains("bin"))
```

### Plotting initial tree
```{r}
cart_NuBi <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi)
```

### Pruning the tree
We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_NuBi_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_pruned, roundint=FALSE)
```

### Making predictions
```{r eval=FALSE}
#predicting the test set
cart_NuBi_pruned_pred <- predict(cart_NuBi_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_NuBi_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

# 3.1 Analysis - Balanced outcome variable

```{r}
#train control for with upsampling
trCtrl_up <- trainControl(method = "cv",
                       summaryFunction = defaultSummary,
                       classProbs = TRUE,
                       number = 10,
                       sampling = "up"
                       )
```

## 1 KNN - NUT, BIN and TOTAL

```{r}
#using unbalanced training set here as apparently it's not required for KNN for data to be balanced
train_temp <- train_bin

test_temp <- test_bin
```

### Tuning K
Best K if we look at both accuracy and Kappa is 115.
```{r}
set.seed(12)
knn_all_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl_up,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(115, 120,by = 1))#initially checked on 1-151 range, best was 115
                )
knn_all_up

set.seed(12)
knn_all_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl_up,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(116, 121,by = 1))#initially checked on 1-151 range, best was 115
                )
knn_all_up
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_all_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 115
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_all_up_pred <- predict(knn_all_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_all_up_pred, positive="good")
```

## 2 KNN - only NUT

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 111.
```{r}
set.seed(12)
knn_Nu_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu_up
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_Nu_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 111
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_up_pred <- predict(knn_Nu_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_up_pred, positive="good")
```

## 3 KNN - NUT and TOTAL
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 139
```{r}
set.seed(12)
knn_NuTo_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(135, 145,by = 2))#initially checked on 1-151 range, best was 139
                )
knn_NuTo_up
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuTo_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 139
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_NuTo_up_pred <- predict(knn_NuTo_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_NuTo_up_pred, positive="good")
```

## 4 KNN - NUT and BIN

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 135
```{r}
set.seed(12)
knn_NuBi_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(35, 45,by = 2))#initially checked on 1-151 range, best was 37
                )
knn_NuBi_up
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuBi_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 135
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_Bi_up_pred <- predict(knn_NuBi_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_up_pred, positive="good")
```

## CART - NUT, BIN and TOTAL
```{r}
#using all features
train_temp <- upsamp_tr_bin

test_temp <- test_bin
```

### Plotting initial tree
```{r}
cart_all_up <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_up)
```

### Pruning the tree
We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r}
set.seed(12)
cart_all_up_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_up_pruned, roundint=FALSE)
```

### Making predictions
Classifies everything as good.
```{r}
#predicting the test set
cart_all_up_pruned_pred <- predict(cart_all_up_pruned, newdata=test_temp, type="class")
length(cart_all_up_pruned_pred)
#confusion matrix
confusionMatrix(data=cart_all_up_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

## CART - NUT, BIN
```{r}
#using all features
train_temp <- upsamp_tr_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
    select(all_of(nutritional_values), contains("bin"))
```

### Plotting initial tree
```{r}
cart_NuBi_up <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_up)
```

### Pruning the tree
We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r}
set.seed(12)
cart_NuBi_up_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_up_pruned, roundint=FALSE)
```

### Making predictions
Classifies everything as good.
```{r}
#predicting the test set
cart_NuBi_up_pruned_pred <- predict(cart_NuBi_up_pruned, newdata=test_temp, type="class")
length(cart_all_up_pruned_pred)
#confusion matrix
confusionMatrix(data=cart_NuBi_up_pruned_pred, reference = test_temp$rating_bin, positive="good")
```
