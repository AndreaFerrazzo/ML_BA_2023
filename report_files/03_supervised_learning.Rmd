# Supervised Learning

## Data Normalisation, splitting and balancing
Creating a new dataframe for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features. We name it `recipes_analysis`.

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))
  
###### CAREFUL --> recipes_analysis should be of dim 10163 x 33
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(across(all_of(contains("bin")), as.factor) , ID = as.character(ID)) %>% 
  select(rating, all_of(nutritional_values), contains("bin"), contains("total"))
```

Creating a normalized version of `recipes_analysis` where the continuous numerical values are normalised, to remain consistent across all models for our analysis.

```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

recipes_analysis_normd <- recipes_analysis %>% 
  mutate(rating = as.factor(rating), across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split

```{r}
#creating training and test sets
set.seed(12)
index <- createDataPartition(recipes_analysis_normd$rating, p=0.75, list = FALSE)
```

We create a training set with the original `rating` variable with 7 levels. 

Below we can see the multilevel rating training set is really unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_multi <- recipes_analysis_normd[index, ]
test_multi <- recipes_analysis_normd[-index, ]

table(train_multi$rating)
```


### Train/test sets with variable combinations for multilevel rating
```{r}
#for train multi
tr_multi_Nut <- train_multi %>% 
  select(rating, all_of(nutritional_values))
te_multi_Nut <- test_multi %>% 
  select(rating, all_of(nutritional_values))

tr_multi_NuBi <- train_multi %>% 
  select(rating, all_of(nutritional_values), all_of(contains("bin")))
te_multi_NuBi <- test_multi %>% 
  select(rating, all_of(nutritional_values), all_of(contains("bin")))

tr_multi_NuTo <- train_multi %>% 
  select(rating, all_of(nutritional_values), all_of(contains("total")))
te_multi_NuTo <- test_multi %>% 
  select(rating, all_of(nutritional_values), all_of(contains("total")))
```

### TrainControl Functions
Here we create the train control functions we will use throughout the project.
```{r trCtrl, cache=FALSE}
#creating train control data for unbalanced data
trCtrl <- trainControl(method = "cv",
                       number = 5)
```

```{r trCtrl_down, cache=FALSE}
#train control with downsampling
trCtrl_down <- trainControl(method = "cv",
                       number = 5,
                       sampling = "down")
```

```{r trCtrl_down_twoClass, cache = FALSE}
#train control with downsampling twoclass summary
trCtrl_down_twoClass <- trainControl(method = "cv",
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE,
                       number = 5,
                       sampling = "down"
                       )
```

## Testing performance on 7-level rating classification with RF models

Given our correlation results in EDA, we anticipated that a classification with 7 levels will basically be equivalent to randomly classifying each observation in one of the 7 rating levels. However, to make sure this is the case, we try running a random forest on the nutritional values, as well as both the "total" and "bin" ingredients-related features we created during the EDA.
We use a random forest for this testing phase because, being an ensemble method, it should give us reasonable results if such results are possible with the data we have. If the results are bad, we can assume that other models will not magically classify everything perfectly.

### RF Multilevel Nut

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Accuracy 41.6
- Kappa 0.047

#### Fitting
We first fit an RF model with multilevel rating on on the nutritional values.
```{r rf_multi_Nut_fit}
set.seed(12)
rf_multi_Nut_fit <- train(rating ~ .,
                     data = tr_multi_Nut,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_Nut_fit
plot(rf_multi_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r rf_multi_Nut_pred}
set.seed(12)
#make predictions
rf_multi_Nut_pred <- predict(rf_multi_Nut_fit, newdata = te_multi_Nut)

#confusion matrix 
confusionMatrix(data = rf_multi_Nut_pred, reference = te_multi_Nut$rating)
```

### RF - Multilevel NuBi

#### Fitting

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Accuracy 45.5
- Kappa 0.001

```{r rf_multi_NuBi_fit}
set.seed(12)
rf_multi_NuBi_fit <- caret::train(rating ~ .,
                     data = tr_multi_NuBi,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_NuBi_fit
plot(rf_multi_NuBi_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
The RF model with the NuBi combination (nutritional values and binary ingredient categories) also classified almost everything in the 4.375 rating class, leading to an accuracy of 45.6% and a Kappa of 1% which indicates that the model is not better than a NIR model. This bias towards the majority class was confirmed by a sensitivity in that class of 98.27%, while all other classes had a sensitivity of 0 or very close to 0.
```{r rf_multi_NuBi_pred}
set.seed(12)
#make predictions
rf_multi_NuBi_pred <- predict(rf_multi_NuBi_fit, newdata = te_multi_NuBi)

#confusion matrix 
confusionMatrix(data = rf_multi_NuBi_pred, reference = te_multi_NuBi$rating)
```

### RF - Multilevel NuTo

#### Fitting 
**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Accuracy 44.8
- Kappa 0.006

```{r rf_multi_NuTo_fit}
set.seed(12)
rf_multi_NuTo_fit <- caret::train(rating ~ .,
                     data = tr_multi_NuTo,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_NuTo_fit
plot(rf_multi_NuTo_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
The final RF model for multilevel rating trained on the NuTo combination (nutritional values and total ingredient categories) did not perform much differently. It has a slightly lower accuracy than the NuBi model at 44.8% with an even worse Kappa at 0.6%. Once again, most observations are classified in the 4.375 rating class.
```{r rf_multi_NuTo_pred}
set.seed(12)
#make predictions
rf_multi_NuTo_pred <- predict(rf_multi_NuTo_fit, newdata = te_multi_NuTo)

#confusion matrix 
confusionMatrix(data = rf_multi_NuTo_pred, reference = te_multi_NuTo$rating)
```

### Evaluation and decision for rest of analysis
Since we see a huge bias towards the majority class in unbalanced data (which we expected), we have to take a decision between 3 options:

- We could try balancing the 7 levels, either up or down. Due to the very large disparity in observation counts per level (min at 32 and max at 3481), we believe than neither up- nor downsampling would be ideal. Indeed, upsampling would mean creating a massive amount of artificial observations through replacement for the minority classes, while downsampling would leave us with an insufficient total amount of observations.
- We could try to aggregate the minority classes into one single class (i.e., putting  1.25, 1.875, 2.5, and 3.125 together), leaving us with 4 final classes. There would still be a significant imbalance however, and given the near-random classification that we observed above, we do not believe aggregating those classes is the solution.
- We could transform the 7 levels into a binary classification problem. This would help both with class imbalance and would hopefully also lead to higher accuracy metrics by simplifying the classification task.

After careful reflection, we believe that the 3rd options is our best bet. We therefore transform the 7-level rating variable into a binary rating variable, with the threshold at 4. This means that all ratings below 4 are now considered as "bad" and all ratings above 4 are considered as "good".

## Combinations of 4 Nutritional + 28 Engineered ingredients variables - unbalanced

```{r summary_fun, cache=FALSE}
#function to create the summary tables from the confusion matrices
summary_table_fun <- function(x){
  acc <- x[[3]][[1]]
  kap <- x[[3]][[2]]
  bacc <- x[[4]][[11]]
  sens <- x[[4]][[1]]
  vec <- c(acc, bacc, sens, kap)
  return(vec)
}

#creating empty summary table for unbalanced data
summary_table_unbal <- tibble(metric = c("accuracy", "balanced_accuracy", "sensitivity", "kappa"))
```

### Data preparation for binary rating analysis

#### Creating new rating_bin variable
We transform the original 7-level rating variable into a binary `rating_bin` variable, "bad" or "good" (i.e., with a threshold at 4 to decide if the rating is bad or good).

```{r}
#creating binary rating variable
recipes_analysis_bin <- recipes_analysis %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  select(-rating)

#reversing rating_bin factor order
recipes_analysis_bin$rating_bin <- factor(recipes_analysis_bin$rating_bin, levels=rev(levels(recipes_analysis_bin$rating_bin )))

#normalising newly created df
recipes_analysis_bin <- recipes_analysis_bin %>% 
  mutate(across(where(is.numeric), my_normalise))
```

#### Creating rating_bin training and test sets
```{r}
#creating training and test sets
set.seed(12)
index_bin <- createDataPartition(recipes_analysis_bin$rating_bin, p=0.75, list = FALSE)
```

We create a training and test set with the new `rating_bin` variable with only 2 levels. 

Below we can see the binary rating training set is still slightly unbalanced throughout the two classes.
```{r}
#creating training and test set with multilevel rating variable
train_bin <- recipes_analysis_bin[index_bin, ]
test_bin <- recipes_analysis_bin[-index_bin, ]

table(train_bin$rating_bin)
```

#### Creating various train/test sets with different variable combinations
```{r}
#creating train and test set for all combinations of variables, for binary rating df
train_bin_Nut <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))
test_bin_Nut <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))

train_bin_NuTo <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
test_bin_NuTo <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

train_bin_NuBi <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_bin_NuBi <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))

train_bin_Tot <- train_bin %>% 
  select(rating_bin, all_of(contains("total")))
test_bin_Tot <- test_bin %>% 
  select(rating_bin, all_of(contains("total")))

train_bin_Bin <- train_bin %>% 
  select(rating_bin, all_of(contains("bin")))
test_bin_Bin <- test_bin %>% 
  select(rating_bin, all_of(contains("bin")))
```


### Logistic Regression - unbalanced - Nut

#### Fitting
We now fit a logreg model on unbalanced data with only the nutritional variables.
```{r logr_Nut_unbal_fit}
set.seed(12)
logr_Nut_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_Nut_unbal_fit
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying all observations as good, resulting in a perfect sensitivity but a 0 specificity and a kappa of 0.

```{r logr_Nut_unbal_pred}
logr_Nut_unbal_pred <- predict(logr_Nut_unbal_fit, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = logr_Nut_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_Nut = summary_table_fun(CM))
```

### Logistic Regression - unbalanced - NuBi

#### Fitting
We now fit a logreg model on unbalanced data with the NuBi variable combination.

```{r logr_NuBi_unbal_fit}
set.seed(12)
logr_NuBi_unbal_fit <- train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_NuBi_unbal_fit
```

#### Predictions

The accuracy here is slightly lower at 58.5%, however, the model now is a bit less biased towards the majority class and classifies at least some obs as bad. Kappa is still very low at 1.9%

```{r logr_NuBi_unbal_pred}
logr_NuBi_unbal_pred <- predict(logr_NuBi_unbal_fit, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = logr_NuBi_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_NuBi = summary_table_fun(CM))
```

### Logistic Regression - unbalanced - NuTo

#### Fitting
We now fit a logreg model on unbalanced data with the NuTo variable combination.
```{r logr_NuTo_unbal_fit}
set.seed(12)
logr_NuTo_unbal_fit <- train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_NuTo_unbal_fit
```

#### Predictions
Accuracy is slightly higher than with the NuBi combination, at 58.9%. The sensitivity is very similar and the kappa is slighty higher at 2.9%
```{r logr_NuTo_unbal_pred}
logr_NuTo_unbal_pred <- predict(logr_NuTo_unbal_fit, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = logr_NuTo_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_NuTo = summary_table_fun(CM))
```
Out of the 3 LogReg models with unbalanced data, the NuTo model seems to perform the best.
### KNN - unbalanced - Nut

#### Fitting
We fit a KNN model on unbalanced data with only the nutritional variables. The best hyperparameter K if we look at both accuracy and Kappa is 113.

```{r knn_Nut_unbal_fit}
set.seed(12)
knn_Nut_unbal_fit <- caret::train(rating_bin ~.,
                                  data = train_bin_Nut,
                                  method = "knn",
                                  trControl = trCtrl,
                                  tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 113
                                  )
knn_Nut_unbal_fit
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r knn_Nut_unbal_pred}
knn_Nut_unbal_pred <- predict(knn_Nut_unbal_fit, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = knn_Nut_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_Nut = summary_table_fun(CM))
```

### KNN - unbalanced - NuBi

#### Fitting
We fit a KNN model on unbalanced data with the NuBi variable combination. The best hyperparameter K if we look at both accuracy and Kappa is 135
```{r knn_NuBi_unbal_fit}
set.seed(12)
knn_NuBi_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "knn",
                trControl = trCtrl,
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi_unbal_fit
```

#### Predictions

Once again most obs are classified as good. Not better than NIR with an accuracy of 58.4 but balanced accuracy of 51.3.

```{r knn_NuBi_unbal_pred}
knn_NuBi_unbal_pred <- predict(knn_NuBi_unbal_fit, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = knn_NuBi_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_NuBi = summary_table_fun(CM))
```

### KNN - unbalanced - NuTo

#### Fitting
We fit a KNN model on unbalanced data with the NuTo variable combination. The best hyperparameter K if we look at both accuracy and Kappa is 147.


```{r knn_NuTo_unbal_fit}
set.seed(12)
knn_NuTo_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "knn",
                trControl = trCtrl,
                tuneGrid= data.frame(k = seq(141, 151,by = 2))#initially checked on 1-151 range, best was 147
                )
knn_NuTo_unbal_fit
```

#### Predictions

We see a balanced accuracy of 51.8 and still a strong bias towards the positive class.

```{r knn_NuTo_unbal_pred}
knn_NuTo_unbal_pred <- predict(knn_NuTo_unbal_fit, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = knn_NuTo_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_NuTo = summary_table_fun(CM))
```

### CART - unbalanced -  Nut

#### Fitting
Fitting a CART model on only nutritinal variables.
```{r cart_nut_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_Nut_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_Nut,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_Nut_unbal_fit
```

The best CP is 0.00305 before reaching a plateau. with higher values of CP. It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nut_unbal_results}
# Get the results for each CP step
cart_Nut_unbal_results <- cart_Nut_unbal_fit$results

cart_Nut_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
Model predicts only positive class.
```{r cart_nut_unbal_pred}
#Now using the cv to predict test set
cart_Nut_unbal_pred <- predict(cart_Nut_unbal_fit, newdata = test_bin_Nut)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_Nut_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_Nut = summary_table_fun(CM))
```

### CART - unbalanced - NuBi

#### Fitting
Fitting a CART model on the NuBi combination.
```{r cart_nubi_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuBi_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_NuBi,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_NuBi_unbal_fit
```

Best CP of 0.0055
```{r cart_nubi_unbal_results}
# Get the results for each CP step
cart_NuBi_unbal_results <- cart_NuBi_unbal_fit$results

cart_NuBi_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
Balanced accuracy is higher than with the CART model including only the Nutritional values.
```{r cart_nubi_unbal_pred}
#Now using the cv to predict test set
cart_NuBi_unbal_pred <- predict(cart_NuBi_unbal_fit, newdata = test_bin_NuBi)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = cart_NuBi_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_NuBi = summary_table_fun(CM))
```

### CART - unbalanced - NuTo

#### Fitting
We now fit a CART with the NuTo variable combination
```{r cart_NuTo_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuTo_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_NuTo,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_NuTo_unbal_fit
```

Again, it might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_NuTo_unbal_results}
# Get the results for each CP step
cart_NuTo_unbal_results <- cart_NuTo_unbal_fit$results

cart_NuTo_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
The model once again classifies everything as good due to the strong majority class bias.
```{r cart_NuTo_unbal_pred}
#Now using the cv to predict test set
cart_NuTo_unbal_pred <- predict(cart_NuTo_unbal_fit, newdata = test_bin_NuTo)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = cart_NuTo_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_NuTo = summary_table_fun(CM))
```

## Combinations of 4 Nutritional + 28 Engineered ingredients variables - balanced

```{r summary_table_down_create}
#creating empty summary table for balanced data
summary_table_down <- tibble(metric = c("accuracy", "balanced_accuracy", "sensitivity", "kappa"))
```

### Random Forest - balanced - Nut

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Balanced Accuracy 53.7
- Sensitivity 53.7
- Kappa 7.2

#### Fitting
```{r rf_nut_fit}
set.seed(12)
rf_Nut_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Nut,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Nut_fit
plot(rf_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
We see a balanced accuracy and sensitivity both equal to 53.7.

```{r rf_nut_pred}
set.seed(12)
#make predictions
rf_Nut_pred <- predict(rf_Nut_fit, newdata = test_bin_Nut)

#confusion matrix 
CM <- confusionMatrix(data = rf_Nut_pred, reference = test_bin_Nut$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_Nut = summary_table_fun(CM))
```

### Random Forest - balanced - NuBi

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- Balanced accuracy 54.2
- Sensitivity 57.7
- Kappa 8.20


#### Fitting
The best mtry is 3.
```{r rf_nubi_fit}
set.seed(12)
rf_NuBi_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuBi,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuBi_fit
plot(rf_NuBi_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
The balanced accuracy and sensitivity are higher than for the model with only nutritional data.
```{r rf_nubi_pred}
set.seed(12)
#make predictions
rf_NuBi_pred <- predict(rf_NuBi_fit, newdata = test_bin_NuBi)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuBi_pred, reference = test_bin_NuBi$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuBi = summary_table_fun(CM))
```

### Random Forest - balanced - NuTo

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Balanced accuracy 55.6
- Sensitivity 59.1
- Kappa 11.1


#### Fitting
```{r rf_nuto_fit}
set.seed(12)
rf_NuTo_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuTo,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuTo_fit
plot(rf_NuTo_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
The balanced accuracy has increased again compared to the NuBi model, now sitting at 55.6. We also reached the highest sensitivity of the 3 RF models tested so far at 59.1.
```{r rf_nuto_pred}
set.seed(12)
#make predictions
rf_NuTo_pred <- predict(rf_NuTo_fit, newdata = test_bin_NuTo)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuTo_pred, reference = test_bin_NuTo$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuTo = summary_table_fun(CM))
```

### Random Forest - balanced - Tot

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- Balanced Accuracy 52.4
- Sensitivity 52.9
- Kappa 4.7


#### Fitting
```{r rf_tot_fit}
set.seed(12)
rf_Tot_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Tot,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Tot_fit
plot(rf_Tot_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
Both balanced accuracy and sensitivity are lower then for the NuTo model. No improvements here.
```{r rf_tot_pred}
set.seed(12)
#make predictions
rf_Tot_pred <- predict(rf_Tot_fit, newdata = test_bin_Tot)

#confusion matrix 
CM <- confusionMatrix(data = rf_Tot_pred, reference = test_bin_Tot$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Tot = summary_table_fun(CM))
```

### Random Forest - balanced - Bin

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- Balanced accuracy 52.6
- Sensitivity 55.7
- Kappa 5.1


#### Fitting
```{r rf_bin_fit}
set.seed(12)
rf_Bin_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Bin,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Bin_fit
plot(rf_Bin_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
We have higher performance. than with the model using only Tot, but still not higher than NuTO
```{r rf_bin_pred}
set.seed(12)
#make predictions
rf_Bin_pred <- predict(rf_Bin_fit, newdata = test_bin_Bin)

#confusion matrix 
CM <- confusionMatrix(data = rf_Bin_pred, reference = test_bin_Bin$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Bin = summary_table_fun(CM))
```

We see that the 3 best performing models are: Nut, NuBi and NuTo. Which was expected given the importance of the nutritional values that we saw in the PCA during the EDA. Therefore, we decide to test further models only with those 3 combinations of variables, thus leaving out models trained solely on Tot or on Bin.


### Logistic Regression - balanced - Nut

**Accuracy metrics**

- Balanced accuracy 51.0
- Sensitivity 34.0
- Kappa 1.8

#### Fitting
Fitting a logreg model on balanced data with only nutritional variables.
```{r logr_Nut_blncd}
set.seed(12)
logr_Nut_blncd <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_Nut_blncd
```
We observe that the CV accuracy is 49.6%.

#### Predictions
We obtain a balanced accuracy of 51.0 and a very low sensitivity at 34.0. No overfitting when compared to the CV accuracy.
```{r logr_Nut_pred_blncd}
logr_Nut_pred_blncd <- predict(logr_Nut_blncd, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = logr_Nut_pred_blncd, positive="good")
CM

summary_table_down <- cbind(summary_table_down, LogReg_Nut = summary_table_fun(CM))
```

### Logistic Regression - balanced - NuBi

**Accuracy metrics**

- Balanced accuracy 54.5
- Sensitivity 55.3
- Kappa 8.8

#### Fitting
Fitting a logreg model on balanced data with NuBi variable combination.
```{r logr_NuBi_blncd}

set.seed(12)
logr_NuBi_blncd <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_NuBi_blncd
```
We observe that the CV accuracy stands at 53.9%.

#### Predictions
Both balanced accuracy at 54.5 and sensitivity are higher than for the model with only Nut variables. Again no overfitting when comparing CV and test accuracy.
```{r logr_NuBi_pred_blncd}
logr_NuBi_pred_blncd <- predict(logr_NuBi_blncd, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = logr_NuBi_pred_blncd, positive="good")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, LogReg_NuBi = summary_table_fun(CM))
```
### Logistic Regression - balanced - NuTo

**Accuracy metrics**

- Balanced accuracy 55.9
- Sensitivity 57.2
- Kappa 11.4

#### Fitting
Fitting a logreg model on balanced data with NuTo variable combination.
```{r logr_NuTo_blncd}

set.seed(12)
logr_NuTo_blncd <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_NuTo_blncd
```
We observe that the CV accuracy set stands at 54.1%.

#### Predictions
We want to evaluate the quality of the model and we notice that the balanced accuracy of the test set stands at 55.9%, which is higher than for the NuBi model fitted above. Same for the sensitivity which is now up to 57.2. Once again no overfitting of the model.
```{r logr_NuTo_pred_blncd}
logr_NuTo_pred_blncd <- predict(logr_NuTo_blncd, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = logr_NuTo_pred_blncd, positive="good")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, LogReg_NuTo = summary_table_fun(CM))
```
The best LogReg model on balanced data is the one with the NuTo combination of variables.

### SVM - balanced - Nut

#### Linear SVM - balanced - Nut

```{r svm_Linear_nutritional, eval=FALSE}

svm_Linear_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=train_bin_Nut, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_nutritional

```

The validation accuracy stands at 42.8%, which is not satisfying considering that it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_nutritional, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_nutritional

plot(svm_Linear_Grid_nutritional)

svm_Linear_Grid_nutritional$bestTune

```

The result indicates that setting the cost to C=10 provides the best model with accuracy=42.9%. The accuracy apparently reaches a plateau at this value. There is no sensible improvement compared to the previous linear SVM with default parameter cost C=1.

#### Radial SVM - balanced - Nut

```{r svm_Radial_nutritional, eval=FALSE}

svm_Radial_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_nutritional

```
The validation accuracy stands at 54.8%, which is substantially better than the SVM model with the linear kernel. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_nutritional, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_nutritional

plot(svm_Radial_Grid_nutritional)

svm_Radial_Grid_nutritional$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=52.6%.

#### SVM Nut - Radial Kernel Best model

```{r recipe_rb_tuned_nutritional}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.1, C = 1000)


recipe_rb_tuned_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_nutritional}
recipe_rb_tuned_pred_nutritional <- predict(recipe_rb_tuned_nutritional, newdata = test_bin_Nut)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_nutritional, reference = test_bin_Nut$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_Nut = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52.9% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1000 and sigma=0.01 is the best model.

### SVM - balanced - NuBi

#### Linear SVM - balanced - NuBi

```{r svm_Linear_NuBi, eval=FALSE}

svm_Linear_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_NuBi

```

The validation accuracy stands at 56.1%. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_NuBi, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_NuBi

plot(svm_Linear_Grid_NuBi)

svm_Linear_Grid_NuBi$bestTune

```

The result indicates that setting the cost to C=0.01 provides the best model with accuracy=57.6%. There is a sensible improvement compared to the previous linear SVM with default parameter cost C=1.


#### Radial SVM - balanced - NuBi

```{r svm_Radial_NuBi, eval=FALSE}

svm_Radial_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_NuBi

```
The validation accuracy stands at 53.7%, which is worse than the SVM model with the linear kernel. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_NuBi, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_NuBi

plot(svm_Radial_Grid_NuBi)

svm_Radial_Grid_NuBi$bestTune

```

The optimal model from this search is with sigma = 0.01 and C = 1. This optimal model would then reach accuracy=54.8%.

#### SVM NuBi - Radial Kernel Best model

```{r recipe_rb_tuned_NuBi}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.01, C = 1)


recipe_rb_tuned_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_NuBi}
recipe_rb_tuned_pred_NuBi <- predict(recipe_rb_tuned_NuBi, newdata = test_bin_NuBi)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_NuBi, reference = test_bin_NuBi$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_NuBi = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 54.1% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1 and sigma=0.01 is the best model.


### SVM - balanced - NuTo

#### Linear SVM - balanced - NuTo

```{r svm_Linear_NuTo, eval=FALSE}

svm_Linear_NuTo <- train(rating_bin ~ calories + protein + fat + sodium, data=train_bin_NuTo, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_NuTo

```

The validation accuracy stands at 42.8%, which is not satisfying considering that it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_NuTo, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_NuTo

plot(svm_Linear_Grid_NuTo)

svm_Linear_Grid_NuTo$bestTune

```

The result indicates that setting the cost to C=1000 provides the best model with accuracy=54%. There is a considerable improvement with respect to the previous linear SVM with default parameter cost C=1.

#### Radial SVM - balanced - NuTo

```{r svm_Radial_NuTo, eval=FALSE}

svm_Radial_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_NuTo

```
The validation accuracy stands at 53.6%, which is substantially better than the SVM model with the linear kernel and default parameters. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_NuTo, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_NuTo

plot(svm_Radial_Grid_NuTo)

svm_Radial_Grid_NuTo$bestTune

```

The optimal model from this search is with sigma = 0.01 and C = 1 This optimal model would then reach accuracy=53.7% .


#### SVM NuTo - Radial Kernel Best model

```{r recipe_rb_tuned_NuTo}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.01, C = 1)


recipe_rb_tuned_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_NuTo}
recipe_rb_tuned_pred_NuTo <- predict(recipe_rb_tuned_NuTo, newdata = test_bin_NuTo)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_NuTo, reference = test_bin_NuTo$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_NuTo = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 55.4% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1 and sigma=0.01 is the best model.

### KNN - balanced - Nut

**Hyperparameters**

Best K = 146

**Accuracy metrics**

- Balanced Accuracy 53.6
- Sensitivity 56.1
- Kappa 7.1

#### Fitting
We fit a KNN model on nutritional variables. Best tuned K parameter is at 146.
```{r knn_nut_fit}
set.seed(12)
knn_Nut <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))
                )
knn_Nut
```

#### Predictions
THe balanced accuracy is at 53.6 with a sensitivity of 56.1. No overfitting when looking at CV accuracy and test set accuracy.
```{r knn_nut_pred}
knn_Nut_pred <- predict(knn_Nut, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = knn_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, KNN_Nut = summary_table_fun(CM))
```

### KNN - balanced - NuBi

**Hyperparameters**

Best K = 71

**Accuracy metrics**

- Balanced Accuracy 51.7
- Sensitivity 51.3
- Kappa 3.4

#### Fitting
We fit a KNN model on balanced data and NuBi variable combination. Best tuned K parameter is at 71
```{r knn_nubi_fit}
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))
                )
knn_NuBi
```

#### Predictions
Performance of the NuBi KNN model is lower than for the model with only Nut variables. No overfitting detected though.
```{r knn_nubi_pred}
knn_NuBi_pred <- predict(knn_NuBi, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = knn_NuBi_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuBi = summary_table_fun(CM))
```

### KNN - balanced - NuTo

**Hyperparameters**

Best K = 136

**Accuracy metrics**

- Balanced Accuracy 53.1
- Sensitivity 47.1
- Kappa 5.9

#### Fitting
We fit a KNN model on balanced data and NuTo variable combination. Best tuned K parameter is at 71
```{r knn_nuto_fit}
set.seed(12)
knn_NuTo <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))
                )
knn_NuTo
```

#### Predictions
The NuTo model exhibits better balanced accuracy and sensitivity than the NuBi model tested above. However, it still had a lower balanced accuracy than the Nut KNN model we tested first, and the sensitivity at 47.1 is rather low as well. No overfitting was detected however.
```{r knn_nuto_pred}
knn_NuTo_pred <- predict(knn_NuTo, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = knn_NuTo_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuTo = summary_table_fun(CM))
```
Overall, the best KNN model is the one fitted only on Nutritional variables.

### CART - balanced -  Nut

**Hyperparameters**

Best parameter CP is 0.0035

**Accuracy metrics**

- Balanced Accuracy 53.4
- Sensitivity 60.9
- Kappa 6.8

#### Fitting
Fitting a CART model with only nutritional variables on balanced data.
```{r cart_nut_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_Nut_fit <- train(rating_bin ~ .,
                  data = train_bin_Nut,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_Nut_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nut_results}
# Get the results for each CP step
cart_Nut_results <- cart_Nut_fit$results

cart_Nut_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions

```{r cart_nut_pred}
#Now using the cv to predict test set
cart_Nut_pred <- predict(cart_Nut_fit, newdata = test_bin_Nut)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_Nut = summary_table_fun(CM))
```

### CART - balanced - NuBi

**Hyperparameters**

Best parameter CP is 0.0055

**Accuracy metrics**

- Balanced Accuracy 53.5
- Sensitivity 54.5
- Kappa 6.8

#### Fitting
Fitting a CART model with NuBi variable combination on balanced data.
```{r cart_nubi_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuBi_fit <- train(rating_bin ~ .,
                  data = train_bin_NuBi,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuBi_fit
```

Best cp of 0.0055 for this model.
```{r cart_nubi_results}
# Get the results for each CP step
cart_NuBi_results <- cart_NuBi_fit$results

cart_NuBi_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
Balanced accuracy at 53.5 is slightly higher than for the Nut model, despite a significantly lower sensitivity. No overfitting detected when comparing CV and test accuracy metrics.
```{r cart_nubi_pred}
#Now using the cv to predict test set
cart_NuBi_pred <- predict(cart_NuBi_fit, newdata = test_bin_NuBi)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuBi_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuBi = summary_table_fun(CM))
```

### CART - balanced - NuTo

**Hyperparameters**

Best parameter CP is 0.00257

**Accuracy metrics**

- Balanced Accuracy 54.0
- Sensitivity 52.3
- Kappa 7.7

#### Fitting
Fitting a CART model with NuTo variable combination on balanced data.
```{r cart_nuto_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuTo_fit <- train(rating_bin ~ .,
                  data = train_bin_NuTo,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuTo_fit
```
CP quickly reaches a plateau, with best parameter at 0.0026.
```{r cart_nuto_results}
# Get the results for each CP step
cart_NuTo_results <- cart_NuTo_fit$results

cart_NuTo_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
THe NuTo CART model reaches the highest balanced accuracy of the 3 CART models on balanced data, with a value of 54%. No overfitting detected.
```{r cart_nuto_pred}
#Now using the cv to predict test set
cart_NuTo_pred <- predict(cart_NuTo_fit, newdata = test_bin_NuTo)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuTo_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuTo = summary_table_fun(CM))
```
The NuTo CART model is the best of the 3 CART models in terms of balanced accuracy, but has the lowest sensitivity of the 3.

## All variables (Nut + 288 ingredients)

### Creating dataset
We first create a new dataset containing the 4 nutritional variables and the 288 original binary ingredient variables (was 293 but we remove 5 below which were present in none of the recipes).
```{r}
analysis_all <- recipes %>% 
  select(rating, all_of(c(nutritional_values, all_ingredients))) %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good")), across(all_of(all_ingredients), as.factor)) %>% 
  select(-rating) %>% 
  select(rating_bin, everything())

#reversing rating_bin factor order
analysis_all$rating_bin <- factor(analysis_all$rating_bin, levels=rev(levels(analysis_all$rating_bin)))

#normalising newly created df
analysis_all <- analysis_all %>% 
  mutate(across(where(is.numeric), my_normalise))
```

We create a training set with the new `rating_bin` variable with only 2 levels, including the nutritional and all ingredients dummies, instead of the engineered variables used in the sub-section above.

```{r}
#creating training and test sets
set.seed(12)
index_all <- createDataPartition(analysis_all$rating_bin, p=0.75, list = FALSE)

#creating training and test set with binary rating variable on all variables
train_all <- analysis_all[index_all, ]
test_all <- analysis_all[-index_all, ]

table(train_all$rating_bin)
```

#### Balancing the binary training set with all variables through manual downsampling

```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_all %>%  filter(rating_bin == "good")
tr_bad <- train_all %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_good <- sample(x = 1:nrow(tr_good), size = nrow(tr_bad), replace = FALSE)
downsamp_tr_all <- tibble(rbind(tr_good[index_good,], tr_bad))

#checking that we have the correct number of good and bad
table(downsamp_tr_all$rating_bin)
```

### Random Forest - balanced - 4 Nut + 288 variables

**Hyperparameters**

Best tune: mtry = 147

**Accuracy metrics**
With tuneLength = 3

- Balanced Accuracy 56.1
- Sensitivity 55.5
- Kappa 11.8

#### Fitting
We fit a RF model on the 292 variables using a tuneLength of 3 for the Mtry parameter to avoid significant computation time with higher tuning lengths.
```{r rf_all_fit}
set.seed(12)
rf_all_fit <- train(rating_bin ~ .,
                    data = train_all,
                    method = "rf",
                    trControl = trCtrl_down,
                    tuneLength = 3)
#show the random forest with cv
rf_all_fit
plot(rf_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
The model achieves a very high balanced accuracy relative to the other models tested so far, at 56.1%. The specificity is also relatively high at 55.5%, but we have seen better in models we have tuned so far. Overall the performance of the RF_all model is still more than decent. We also checked that there was not overfitting by comparing the CV accuracy at mtry 147 with the test accuracy, both being at around 56%.
```{r rf_all_pred}
set.seed(12)
#make predictions
rf_all_pred <- predict(rf_all_fit, newdata = test_all)

#confusion matrix 
CM <- confusionMatrix(data = rf_all_pred, reference = test_all$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_all = summary_table_fun(CM))
```

### Logistic Regression - balanced - 4 Nut + 288 variables
**Accuracy metrics**

- Balanced Accuracy 55.8
- Sensitivity 55.4
- Kappa 11.3

#### Fitting
We now fit a logreg model on the 4 nut variables and the 288 ingredients variables.
```{r logreg_all_fit, warning=FALSE}
#too long
logreg_all_fit <- train(rating_bin~., 
                       data=downsamp_tr_all, 
                       method="glm", 
                       family = "binomial",
                       trControl=trCtrl_down,
                       trace = FALSE)
logreg_all_fit
```

#### Predictions
We find a balanced accuarcy which is slightly lower than the one with the RF_all model, but still relatively higiher. The accuarcy of the LogReg_all model is also very slightly than that the one of the RF_all. Again no overfitting detected for this model which is good.
```{r logreg_all_pred, warning=FALSE}
#predicting
logreg_all_pred <- predict(logreg_all_fit, newdata = test_all)
#creating confusion matrix
CM <- confusionMatrix(reference = test_all$rating_bin, data = logreg_all_pred)
CM
#adding results to the summary_table_unbal
summary_table_down <- cbind(summary_table_down, LogReg_all = summary_table_fun(CM))
```

### CART - balanced - 4 Nut + 288 variables

**Hyperparameters**

Best parameter CP is 0.0036

**Accuracy metrics**

- Balanced Accuracy 53.5
- Sensitivity 52.7
- Kappa 6.8


#### Fitting
```{r cart_all_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_all_fit <- train(rating_bin ~ .,
                  data = train_all,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_all_fit
```
Once again the CP parameter reaches a plateau, even though it might look like the accuracy will keep climbing, it is not the case.
```{r cart_all_results}
# Get the results for each CP step
cart_all_results <- cart_all_fit$results

cart_all_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
The results for the CART model with all variables are rather disappointing. It had a pretty low balanced accuracy and specificity, similar to the CART models trained on the engineered variables and nutritional values combinations. It can't compete with the LogReg_all or RF_all model tested above.
```{r cart_all_pred}
#Now using the cv to predict test set
cart_all_pred <- predict(cart_all_fit, newdata = test_all)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_all_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_all = summary_table_fun(CM))
```

### XGBoost - balanced - 4 Nut + 288 variables

Here we decided to try the XGBoost model for the first time as a bonus test to see if it would magically work better than the others we have tested so far with other variable combinations.

```{r xgb_all datasets, cache=FALSE}
#creating datasets for xgboost
numeric_downsamp_tr <- downsamp_tr_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

numeric_te <- test_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

#creating datasets which don't include the labels, for inputs in the xgb Matrix
explan_downsamp_tr <- numeric_downsamp_tr %>% 
  select(-rating_bin)

explan_te <- numeric_te %>% 
  select(-rating_bin)

#prepare data for XGBoost by creating xgb matrix
xgb_downsamp_tr <- xgb.DMatrix(data = as.matrix(explan_downsamp_tr), label = numeric_downsamp_tr$rating_bin)

xgb_te <- xgb.DMatrix(data = as.matrix(explan_te), label = numeric_te$rating_bin)
```


#### Fitting

**Accuracy metrics**

- Balanced Accuracy 56.0
- Sensitivity 59.5
- Kappa 11.8

We fit the XGBoost model after manual tuning of the hyperparameters.
```{r xgb_all_down_fit, cache=FALSE}
#set hyperparameters
param_list <- list(
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 7,
  max_depth = 5,
  min_child_weight = 1,
  subsample = 0.6,
  colsample_bytree = 0.6
)

#train the model
set.seed(12)
xgb_down_fit <- xgboost(data = xgb_downsamp_tr,
                     params = param_list,
                     nrounds = 100,
                     verbose = FALSE)
```

Here we control for overfitting by checking the balanced accuracy on the training set, which stands at a 61.3%. We have found a great balance between performance and overfitting by setting the gamma parameter at 7 and the nrounds at 100.
```{r xgb_down_prob_train}
#testing on training set to control overfitting
xgb_down_prob_train <- predict(xgb_down_fit, newdata = xgb_downsamp_tr)

xgb_down_pred_train <- as.factor(ifelse(xgb_down_prob_train<0.5, 0, 1))

CM <- confusionMatrix(reference = as.factor(numeric_downsamp_tr$rating_bin), data = xgb_down_pred_train, positive = "1")
CM
```

#### Predictions
We now check the actual balanced accuracy on the testing set. We see that the accuracy is at 56.0%. While this is slightly lower than what we found on the training set, it does not signal significant overfitting.

In terms of other metrics, we achieve a relatively high balanced accuracy at 56.0 which is just slightly lower than the BACC found with the RF_all model. We also have a high sensitivity at 59.5. This model looks like a great candidate to be one of the best models overall.
```{r xgb_all_down_pred, cache=FALSE}
xgb_down_prob <- predict(xgb_down_fit, newdata = xgb_te)

xgb_down_pred <- as.factor(ifelse(xgb_down_prob<0.5, 0, 1))

CM <- confusionMatrix(reference = as.factor(numeric_te$rating_bin), data = xgb_down_pred, positive = "1")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, XGB_all = summary_table_fun(CM))
```

## Model Selection
We decide to select models only within the models that we fitted with the binary rating variable. This leaves us with both the balanced and unbalanced sections with the binary rating.

```{r}
#creating new df formats for model selection analysis plots
summary_down_long <- summary_table_down %>% 
  filter(metric != "accuracy") %>% 
  pivot_longer(-metric, names_to = "model", values_to = "value") %>% 
  select(model, everything()) %>% 
  mutate(value = value*100) %>% 
  arrange(model)

summary_down_wide <- summary_down_long %>% 
  pivot_wider(names_from = "metric", values_from = "value")
```

### Unbalanced data
Here we can see the results for section 5.3 on the unbalanced data. We observe that the balanced accuracy is rarely too far from 50% which indicates a pretty bad classification. The sensitivity is also very high for most models due to the bias towards the majority class "good" due to the class imbalance. Given this poor performance, we decide to focus on the models with balanced data to perform our selection of the best model.
```{r summary_table_unbal}
#displaying table without accuracy metric since we do not use it in the model selection.
summary_table_unbal %>%
  filter(metric != "accuracy") %>% 
  kbl() %>% 
  kable_styling(position = "center")
```

### Balanced data
Here we can see the results for section 5.4 on the downsampled data, with a total of 21 models that we fitted with various combinations of explanatory variables. We see that the balanced accuracy is already higher, and the sensitivity is more reasonable. Below, we investigate various aspects of these metrics and select the top-performing models. We decide to use balanced accuracy and sensitivity as the two core metrics to quantitatively rank the best models.
```{r summary_table_down}
#displaying table without accuracy metric since we do not use it in the model selection.
summary_down_wide %>% 
  kbl() %>% 
  kable_styling(position = "center")

summary_table_down_store <- summary_table_down
summary_table_down <- summary_table_down[, 1:22]
```


```{r}
#selecting 3 models with highest balance accuracy
bacc_select <- summary_down_wide %>% 
  arrange(desc(balanced_accuracy)) %>% 
  dplyr::slice(1:5) %>% pull(model)
#selecting 3 models with highest sens
sens_select <- summary_down_wide %>% 
  arrange(desc(sensitivity)) %>% 
  dplyr::slice(1:5) %>% pull(model)
#selecting 3 models with highest kappa
kappa_select <- summary_down_wide %>% 
  arrange(desc(kappa)) %>% 
  dplyr::slice(1:5) %>% pull(model)
```

We first look at the top-5 models with the highest balanced_accuracy
```{r}
mean_bacc <- summary_down_wide %>% 
  summarise(mean_bacc = mean(balanced_accuracy)) %>% pull() %>% round(., 2)
            
summary_down_long %>% 
  filter(model %in% bacc_select, metric=="balanced_accuracy") %>% 
  ggplot(aes(x=model, y=value, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(50, 60)) + 
  scale_fill_manual(values=c("#F8766D", "#C49A00", "#00C094", "#A58AFF", "#FB61D7"))+
  labs(y = "Balance accuracy (in %)", x = "Model", title = "Top 5 models with highest balanced accuracy")
```

We now plot the top-5 models with the highest sensitivity. 
```{r}
mean_sens<- summary_down_wide %>% 
  summarise(mean_sens = mean(sensitivity)) %>% pull() %>% round(., 2)

summary_down_long %>% 
  filter(model %in% sens_select, metric=="sensitivity") %>% 
  ggplot(aes(x=model, y=value, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(55, 65)) + 
  scale_fill_manual(values=c("#53B400", "#C49A00", "#00B6EB", "#A58AFF", "#FB61D7"))+
  labs(y = "Sensitivity (in %)", x = "Model", title = "Top 5 models with highest sensitivity")
```

We then create a ranking system with points based on the position of each model regarding the two core metrics, balanced accuracy and sensitivity.
```{r}
summary_points <- summary_down_wide %>% 
  mutate(rank_bacc = 22-rank(balanced_accuracy), points_bacc = rank(balanced_accuracy), rank_sens = 22-rank(sensitivity), points_sens = rank(sensitivity), total_points = (points_bacc + points_sens))

summary_points %>% 
  arrange(desc(total_points)) %>%
  dplyr::slice(1:5) %>% 
  select(model, points_bacc, points_sens, total_points) %>% 
  kbl() %>% 
  kable_styling(position = "center")
  
summary_points %>% 
  arrange(desc(total_points)) %>% 
  dplyr::slice(1:5) %>%
  ggplot(aes(x=model, y=total_points, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(30, 40)) + 
  scale_y_continuous(breaks = seq(30, 40, by = 1))+
  scale_fill_manual(values=c("#C49A00", "#00C094", "#00B6EB", "#A58AFF", "#FB61D7"))+
  labs(y = "Total number of points", x = "Model", title = "Top 5 models with highest number of points")
```
Here we can see that XGB_all has the highest number of points based on the weighted ranking system combining the balanced accuracy and sensitivity metrics with a total of 40 points. Followed by the RF_NuTo and LogReg_NuTo sharing the second place with 36 points.

### Showcasing the best model - XGBoost_all
Here we show 4 metrics for the best model we select. We can see that the balanced accuracy is at 56%. The sensitivity is the highest at around 59.5%. Kappa is at 11.85% which is pretty low but still in the top values compared to other models we compared.
```{r}
summary_down_long %>% 
  filter(model =="XGB_all") %>% 
  ggplot(aes(x=model, y=value, fill = metric))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(10, 60)) + 
  labs(y = "Value (in %)", x = "", title = "Showcasing 4 metrics for the XGB_all model")+
  scale_fill_viridis(discrete = TRUE)
```

#### Prediction robustness and plotting ROC curve of XGBoost_all
We can clearly see even the best model has a lot of trouble clearly classifying the results by attributing probabilities to each class. There is some severe overlap in probabilites around the threshold at 0.5. The median probability for each class is barely above and below the threshold for "good" and "bad" respectively. This indicates very low robustness of the model and confirms the poor classification performance.

```{r}
#predicting on the testing set
roc_df <- test_all %>% 
  mutate(prob_test = predict(xgb_down_fit, newdata = xgb_te))

#plotting histogram
roc_df %>%
  ggplot(aes(x = prob_test, fill = rating_bin)) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities on the test set - XGB_all")

#plotting boxplot
roc_df %>%
  ggplot(aes(x = rating_bin, y = prob_test)) +
  geom_boxplot() + 
  labs(x = "Response", y = "Probability", title = "Boxplot of predicted probabilities on the test - XGB_all")

```
Here we can see the ROC curve relating to the test set probabilities of the XGB_all model. This shows us that is is better than the random model, but not great peformance either as the AUC is clearly quite small.
```{r}
rocit_emp <- rocit(score = xgb_down_prob, class = numeric_te$rating_bin, method = "emp")

plot(rocit_emp, col = c(1,"gray50"), legend = TRUE, YIndex = TRUE)

```

## Variable Importance

Below we show all the variables from most to least important from the top model we selected, XGB_all.
```{r VarImp XGBoost, fig.height=10, fig.width=8}
#XGBoost variable importance
xgb_importance_matrix <- xgb.importance(model = xgb_down_fit)
xgb_importance_matrix

xgb.ggplot.importance(importance_matrix = xgb_importance_matrix) +
  labs(title = "XGBoost Feature Importance")
```
