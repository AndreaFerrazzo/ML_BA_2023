# Supervised Learning

## 0. Data Normalisation, splitting and balancing
Creating a new dataframe for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features. We name it `recipes_analysis`.

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))

###### CAREFUL --> recipes_analysis should be of dim 10163 x 33
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(across(all_of(contains("bin")), as.factor), ID = as.character(ID)) %>% 
  select(rating, all_of(nutritional_values), contains("bin"), contains("total"))
```

Creating a normalized version of `recipes_analysis` where the continuous numerical values are normalised, to remain consistent across all models for our analysis.

```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

recipes_analysis_normd <- recipes_analysis %>% 
  mutate(rating = as.factor(rating), across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split

```{r}
#creating training and test sets
set.seed(12)
index <- createDataPartition(recipes_analysis_normd$rating, p=0.75, list = FALSE)
```

We create a training set with the original `rating` variable with 7 levels. 

Below we can see the multilevel rating training set is really unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_multi <- recipes_analysis_normd[index, ]
test_multi <- recipes_analysis_normd[-index, ]

table(train_multi$rating)
```


### Creating train and test sets for each variable combination
```{r}
#for train multi
tr_multi_all <- train_multi
te_multi_all <- test_multi

tr_multi_Nut <- train_multi %>% 
  select(rating, all_of(nutritional_values))
te_multi_Nut <- test_multi %>% 
  select(rating, all_of(nutritional_values))
```

## 1.0 Testing performance on 7-level rating classification with RF

Given our correlation results in EDA, we anticipated that a classification with 7 levels will basically be equivalent to randomly classifying each observation in one of the 7 rating levels. However, to make sure this is the case, we try running a random forest on the nutritional values, as well as both the "total" and "bin" ingredients-related features we created during the EDA.
We use a random forest for this testing phase because, being an ensemble method, it should give us reasonable results if such results are possible with the data we have. If the results are bad, we can assume that other models will not magically classify everything perfectly.

```{r}
#creating train control data for unbalanced data
trCtrl <- trainControl(method = "cv",
                       number = 5)
```

```{r}
#creating train control data for unbalanced data
trCtrl_down <- trainControl(method = "cv",
                       number = 5,
                       sampling = "down")
```

```{r}
#creating train control data for unbalanced data
trCtrl_up <- trainControl(method = "cv",
                       number = 5,
                       sampling = "up")
```

### Fitting RF - multi_all

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 45.6
```{r}
set.seed(12)
rf_multi_all_fit <- caret::train(rating ~ .,
                     data = tr_multi_all,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_all_fit
plot(rf_multi_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions RF - multi_all
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r}
#make predictions
rf_multi_all_pred <- predict(rf_multi_all_fit, newdata = test_multi)

#confusion matrix 
confusionMatrix(data = rf_multi_all_pred, reference = test_multi$rating)
```

### Fitting - RF multi_Nut
We now try another RF but only with the nurtitional values as explanatory variables to see if it changes something.

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 41.6

```{r}
set.seed(12)
rf_multi_Nut_fit <- train(rating ~ .,
                     data = tr_multi_Nut,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_Nut_fit
plot(rf_multi_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions - RF multi_Nut

As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r}
#make predictions
rf_multi_Nut_pred <- predict(rf_multi_Nut_fit, newdata = te_multi_all)

#confusion matrix 
confusionMatrix(data = rf_multi_Nut_pred, reference = te_multi_Nut$rating)
```

### Evaluation and decision for rest of analysis
Since we see a huge bias towards the majority class in unbalanced data (which we expected), we have to take a decision between 3 options:

- We could try balancing the 7 levels, either up or down. Due to the very large disparity in observation counts per level (min at 32 and max at 3481), we believe than neither up- nor downsampling would be ideal. Indeed, upsampling would mean creating a massive amount of artificial observations through replacement for the minority classes, while downsampling would leave us with an insufficient total amount of observations.
- We could try to aggregate the minority classes into one single class (i.e., putting  1.25, 1.875, 2.5, and 3.125 together), leaving us with 4 final classes. There would still be a significant imbalance however, and given the near-random classification that we observed above, we do not believe aggregating those classes is the solution.
- We could transform the 7 levels into a binary classification problem. This would help both with class imbalance and would hopefully also lead to higher accuracy metrics by simplifying the classification task.

After careful reflection, we believe that the 3rd options is our best bet. We therefore transform the 7-level rating variable into a binary rating variable, with the threshold at 4. This means that all ratings below 4 are now considered as "bad" and all ratings above 4 are considered as "good".

## 1.1 Engineered variables - unbalanced

### Methodology 

### Creating new rating_bin variable
and one with a engineered rating variable which we converted to binary "bad" or "good" (i.e., with a threshold at 4 to decide if the rating is bad or good).

```{r}
#creating binary rating variable
recipes_analysis_bin <- recipes_analysis%>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  select(-rating)
#normalising newly created df
recipes_analysis_bin <- recipes_analysis_bin %>% 
  mutate(across(where(is.numeric), my_normalise))
```

```{r}
#creating training and test sets
set.seed(12)
index_bin <- createDataPartition(recipes_analysis_bin$rating_bin, p=0.75, list = FALSE)
```

We create a training set with the new `rating_bin` variable with only 2 levels. 

Below we can see the binary rating training set is still slightly unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_bin <- recipes_analysis_bin[index_bin, ]
test_bin <- recipes_analysis_bin[-index_bin, ]

table(train_bin$rating_bin)
```

```{r}
#creating train and test set for all combinations of variables, for binary rating df
train_bin_Nut <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))
test_bin_Nut <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))

train_bin_NuTo <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
test_bin_NuTo <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

train_bin_NuBi <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_bin_NuBi <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

Balancing the binary training set through manual upsampling

```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_bin %>%  filter(rating_bin == "good")
tr_bad <- train_bin %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))

#checking that we have the correct number of good and bad
table(upsamp_tr_bin$rating_bin)
```

### Logistic Regression - unbalanced - Nut

```{r}
# 
# # to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# # unbalanced data
# # 
# train_bin_01 <- train_bin %>%
#   mutate(rating_bin = ifelse(rating_bin=="good",1,0))
# test_bin_01 <- test_bin %>%
#   mutate(rating = ifelse(rating_bin=="good",1,0))

###########
#GIVES THE SAME RESULTS WITH GOOD/BAD and 0/1 as rating_bin, be it as factors or not
```

```{r}

# since the data has already been split we proceed with the fitting of the logistic regression
# nutritional values

rating_logr <- glm(rating_bin ~., data=train_bin_Nut, family="binomial")

summary(rating_logr)

```

#### Variable selection and interpretation

```{r}
rating_logr_sel <- step(rating_logr)
summary(rating_logr_sel)
```

TO REVIEW TEXT:

The variables calories, total_fruits, total_carbs, total_fruits and seafood_bin are statistically significant at alpha=0.01. For instance, we can observe that the probability of having a "good" rating increases with a higher value of calories per recipe, but it decreases with a larger number of food high in carbs per recipe.

The variables total_fish, total_herbs, total_cheese and meats_bin are statistically significant at alpha=0.05. In this case we would see the probability of having a "good" rating increasing with a higher number of cheeses per recipe.

#### Inference

```{r}
prob_te_rating <- predict(rating_logr_sel, newdata = test_bin_Nut, type="response")
pred_te_rating <- as.factor(ifelse(prob_te_rating >= 0.5, "good", "bad"))
table(pred_te_rating)

confusionMatrix(reference = test_bin_Nut$rating_bin, data = prob_te_rating, positive="good")

# table(Pred=pred_te_rating, Obs=test_bin_Nut$rating_bin)
# 
# accuracy_rating <- sum(pred_te_rating == test_bin_Nut$rating_bin) / length(test_bin_Nut$rating_bin)
# 
# print(paste("Accuracy:", accuracy_rating))
```

As we can see the predictions are not satisfying. Since the number of good recipes is larger than the number of bad recipes, predicting a 1 will always provide a good model overall. Anyway the model is not reliable when it comes to predict recipes with a 0.

We believe that it is worth to proceed with the same analysis using balanced data.

### SVM - unbalanced - Nut

Only nutritional values

```{r}

recipe_svm_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium , data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm_nutritional

```

Let us make predictions and check the accuracy

```{r}

recipe_svm_pred_nutritional <- predict(recipe_svm_nutritional, newdata = test_bin)

table(Pred=recipe_svm_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred_nutritional, reference = test_bin$rating_bin)

```

In this case the accuracy stands at 59.1%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the accuracy.

### SVM - unbalanced - NuBi

```{r}
# with the svm() function of the e1071 package we can fit SVM to the data with several possible kernels 
# here we start with the linear kernel

# unbalanced data

train_bin$rating_bin <- as.factor(train_bin$rating_bin)
test_bin$rating_bin <- as.factor(test_bin$rating_bin)


recipe_svm <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm

```

Let us make predictions and check the accuracy

```{r}

recipe_svm_pred <- predict(recipe_svm, newdata = test_bin)

table(Pred=recipe_svm_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred, reference = test_bin$rating_bin)

```

In this case the accuracy stands at 59%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the quality of our analysis.

### KNN - unbalanced - Nut

#### Fitting
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```


Best K if we look at both accuracy and Kappa is 111.

```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_Nu <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_pred <- predict(knn_Nu, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_pred, positive="good")
```

### KNN - unbalanced - NuBi

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

#### Fitting
Best K if we look at both accuracy and Kappa is 135

```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi
```

#### Predictions

Once again most obs are classified as good. Not better than NIR.

```{r}
knn_Bi_pred <- predict(knn_NuBi, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_pred, positive="good")
```

### CART - unbalanced - Nut

```{r}
#using all features
train_temp <- train_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
  select(all_of(nutritional_values), contains("bin"))
```

#### Plotting initial tree

```{r}
cart_all <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_all_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_pruned, roundint=FALSE)
```

#### Making predictions

```{r eval=FALSE}
#predicting the test set
cart_all_pruned_pred <- predict(cart_all_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_all_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

### CART - unbalanced - NuBi

```{r}
#using all features
train_temp <- train_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
  select(all_of(nutritional_values), contains("bin"))
```

#### Plotting initial tree

```{r}
cart_NuBi <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r eval=FALSE}
set.seed(12)
cart_NuBi_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_pruned, roundint=FALSE)
```

#### Making predictions

```{r eval=FALSE}
#predicting the test set
cart_NuBi_pruned_pred <- predict(cart_NuBi_pruned, newdata=test_temp, type="class")

#confusion matrix
confusionMatrix(data=cart_NuBi_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

## 1.2. Engineered variables - balanced

### Logistic Regression - balanced - All variables

```{r}

# to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# the same procedure on the test_bin has already been applied before

upsamp_tr_bin$rating_bin <- ifelse(upsamp_tr_bin$rating_bin=="good",1,0)

```

```{r}

# since the data has already been splitted we proceed with the fitting of the logistic regression
# nutritional values, categories and total ingredients 

rating_logr_up <- glm(rating_bin ~., data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up <- step(rating_logr_up)
summary(rating_logr_sel_up)

```

Among the variables selected, total_carbs, total_fruits, total_meat, seafood_bin and calories are statistically significant at alpha=0.01. For instance, we can observe that the probability of having a "good" rating increases with a higher number of fruits per recipe, but it decreases with a larger number of food high in carbs per recipe. Additionally, the probability of having a "good" rating increases with the content of calories.

#### Inference

```{r}

prob_te_rating_up <- predict(rating_logr_sel_up, newdata = test_bin, type="response")
pred_te_rating_up <- ifelse(prob_te_rating_up >= 0.5, 1, 0)

table(Pred=pred_te_rating_up, Obs=test_bin$rating_bin)

accuracy_rating_up <- sum(pred_te_rating_up == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up))

```

Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has worsened at predicting "good" ratings. The use of balanced data has then proved to be useful.

### Logistic Regression - balanced - Nutri

Focus on nutritional values only

```{r}

rating_logr_up_nutritional <- glm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional <- step(rating_logr_up_nutritional)
summary(rating_logr_sel_up_nutritional)

```

We can observe only two variables which are statistically significant at alpha=0.05. More in particular, we can observe that the probability of having a "good" rating increases with a higher content of fat and sodium per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional <- predict(rating_logr_sel_up_nutritional, newdata = test_bin, type="response")
pred_te_rating_up_nutritional <- ifelse(prob_te_rating_up_nutritional >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional <- sum(pred_te_rating_up_nutritional == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional))

```

In this case the accuracy is lower compared to the model with all variables included.

### Logistic Regression - balanced - NuTo

Focus on nutritional values and total

```{r}

rating_logr_up_nutritional_total <- glm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_vegetables + total_meat + total_fish + total_seafood + total_herbs + total_nuts + total_fruits + total_nuts + total_fruits + total_cheese + total_dairy + total_spices + total_cereals + total_carbs + total_dessert, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_total)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional_total <- step(rating_logr_up_nutritional_total)
summary(rating_logr_sel_up_nutritional_total)

```

Among the variables selected, calories, total_carbs, total_fruits, total_meat, total_seafood are statistically significant at alpha=0.01. The rest of the variables are statistically significant at alpha=0.05. For instance, we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional_total <- predict(rating_logr_sel_up_nutritional_total, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_total <- ifelse(prob_te_rating_up_nutritional_total >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_total, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_total <- sum(pred_te_rating_up_nutritional_total == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_total))

```

Better to add "total" variables to the model rather than using only nutritional values. The accuracy increases from 47.9% to 53.1%.

### Logistic Regression - balanced - NuBi

Focus on nutritional values and bins

```{r}

rating_logr_up_nutritional_bin <- glm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up_nutritional_bin)

```

#### Variable selection and interpretation

```{r}

rating_logr_sel_up_nutritional_bin <- step(rating_logr_up_nutritional_bin)
summary(rating_logr_sel_up_nutritional_bin)

```

Among the variables selected, calories, seafood_bin, fruits_bin, carbs_bin, meats_bin, fish_bin and nuts_bin are statistically significant at alpha=0.01. The rest of the variables, apart from cheese_bin, are statistically significant at alpha=0.05. For instance, also in this case we can observe that the probability of having a "good" rating increases with a higher number of seafood per recipe, but it decreases with a larger number of food high in carbs per recipe.

#### Inference

```{r}

prob_te_rating_up_nutritional_bin <- predict(rating_logr_sel_up_nutritional_bin, newdata = test_bin, type="response")
pred_te_rating_up_nutritional_bin <- ifelse(prob_te_rating_up_nutritional_bin >= 0.5, 1, 0)

table(Pred=pred_te_rating_up_nutritional_bin, Obs=test_bin$rating_bin)

accuracy_rating_up_nutritional_bin <- sum(pred_te_rating_up_nutritional_bin == test_bin$rating_bin) / length(test_bin$rating_bin)

print(paste("Accuracy:", accuracy_rating_up_nutritional_bin))

```

Better to add "bin" variables to the model rather than using nutritional values and total. The accuracy slightly improves from 53.1% to 53.7%.

### Linear SVM - balanced - Nutri

```{r}
upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up_nutritional
```

```{r}

recipe_svm_up_pred_nutritional <- predict(recipe_svm_up_nutritional, newdata = test_bin)

table(Pred=recipe_svm_up_pred_nutritional, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred_nutritional, reference = test_bin$rating_bin)

```

Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 43%. We will proceed the rest of the analysis with balanced data.

#### Tuning the Hyperparameters

```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear_nutritional

```

The validation accuracy stands at 50.9%, which is not that high even though it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid_nutritional

plot(svm_Linear_Grid_nutritional)

```

```{r}

svm_Linear_Grid_nutritional$bestTune

```

The result indicates that setting the cost to C=10 provides the best model with accuracy=50.7%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.

### Radial SVM - balanced - Nutri

```{r}

recipe_rb_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb_nutritional

```

Let us make predictions and check the accuracy

```{r}

recipe_rb_pred_nutritional <- predict(recipe_rb_nutritional, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred_nutritional, reference = test_bin$rating_bin)

```

The accuracy is now 52.5% meaning that the radial basis kernel seems to do much better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.


#### Tuning the Hyperparameters - Radial basis SVM

```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid_nutritional

plot(svm_Radial_Grid_nutritional)

```

```{r}

svm_Radial_Grid_nutritional$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=55.7%.

### SVM Nutri - Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned_nutritional <- svm(rating_bin ~ calories + protein + fat + sodium, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid_nutritional$bestTune$sigma, 
                       cost = svm_Radial_Grid_nutritional$bestTune$C)

recipe_rb_tuned_pred_nutritional <- predict(recipe_rb_tuned_nutritional, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred_nutritional, reference = test_bin$rating_bin)

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52.8% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1000 and sigma=0.01 is the best model.

### Linear SVM - balanced - NuBi

```{r}

upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up

```

```{r}

recipe_svm_up_pred <- predict(recipe_svm_up, newdata = test_bin)

table(Pred=recipe_svm_up_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred, reference = test_bin$rating_bin)

```

Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 57.3%. We will proceed the rest of the analysis with balanced data.

#### Tuning the Hyperparameters

```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear

```

The validation accuracy stands at 53.2%, which is not that high even though it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

```

```{r}

svm_Linear_Grid$bestTune

```

The result indicates that setting the cost to C=1000 provides the best model with accuracy=54.5%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.


### Radial SVM - balanced - NuBi

```{r}

recipe_rb <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb

```

Let us make predictions and check the accuracy

```{r}

recipe_rb_pred <- predict(recipe_rb, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred, reference = test_bin$rating_bin)

```

The accuracy is now 53.3% meaning that the radial basis kernel seems to do slightly better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.

#### Tuning the Hyperparameters 

```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid <- train(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid

plot(svm_Radial_Grid)

```

```{r}

svm_Radial_Grid$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=64.2%.

### SVM NuBi - Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned <- svm(rating_bin ~ calories + protein + fat + sodium + vegetables_bin + meats_bin + fish_bin + seafood_bin + herbs_bin + nuts_bin + fruits_bin + cheese_bin + dairy_bin + spices_bin + cereals_bin + carbs_bin + dessert_bin + egg_bin, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid$bestTune$sigma, 
                       cost = svm_Radial_Grid$bestTune$C)

recipe_rb_tuned_pred <- predict(recipe_rb_tuned, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred, reference = test_bin$rating_bin)

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 54.3% on the test set. We can conclude that among the models , the linear kernal SVM without tuned hyperparameters represents the best model with 57.3% accuracy.

```{r}
#train control for with upsampling
trCtrl_up <- trainControl(method = "cv",
                       summaryFunction = defaultSummary,
                       classProbs = TRUE,
                       number = 10,
                       sampling = "up"
                       )
```

### KNN - NUT, BIN and TOTAL

```{r}
#using unbalanced training set here as apparently it's not required for KNN for data to be balanced
train_temp <- train_bin

test_temp <- test_bin
```

#### Tuning K

Best K if we look at both accuracy and Kappa is 115.

```{r}
set.seed(12)
knn_all_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl_up,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(115, 120,by = 1))#initially checked on 1-151 range, best was 115
                )
knn_all_up

set.seed(12)
knn_all_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl_up,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(116, 121,by = 1))#initially checked on 1-151 range, best was 115
                )
knn_all_up
```

##### Roc curve

We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.

```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_all_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

#### Fitting cv model with K = 115

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_all_up_pred <- predict(knn_all_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_all_up_pred, positive="good")
```

### KNN - only NUT

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```

#### Tuning K

Best K if we look at both accuracy and Kappa is 111.

```{r}
set.seed(12)
knn_Nu_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu_up
```

##### Roc curve

We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.

```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_Nu_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

#### Fitting cv model with K = 111

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_up_pred <- predict(knn_Nu_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_up_pred, positive="good")
```

### KNN - NUT and TOTAL

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
```

#### Tuning K

Best K if we look at both accuracy and Kappa is 139

```{r}
set.seed(12)
knn_NuTo_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(135, 145,by = 2))#initially checked on 1-151 range, best was 139
                )
knn_NuTo_up
```

##### Roc curve

We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.

```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuTo_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

#### Fitting cv model with K = 139

Once again most obs are classified as good. Not better than NIR.

```{r}
knn_NuTo_up_pred <- predict(knn_NuTo_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_NuTo_up_pred, positive="good")
```

### KNN - NUT and BIN

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

#### Tuning K

Best K if we look at both accuracy and Kappa is 135

```{r}
set.seed(12)
knn_NuBi_up <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(35, 45,by = 2))#initially checked on 1-151 range, best was 37
                )
knn_NuBi_up
```

##### Roc curve

We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.

```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuBi_up, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

#### Fitting cv model with K = 135

Once again most obs are classified as good. Not better than NIR.

```{r}
knn_Bi_up_pred <- predict(knn_NuBi_up, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_up_pred, positive="good")
```

### CART - NUT, BIN and TOTAL

```{r}
#using all features
train_temp <- upsamp_tr_bin

test_temp <- test_bin
```

#### Plotting initial tree

```{r}
cart_all_up <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_up)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r}
set.seed(12)
cart_all_up_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_all_up_pruned, roundint=FALSE)
```

#### Making predictions

Classifies everything as good.

```{r}
#predicting the test set
cart_all_up_pruned_pred <- predict(cart_all_up_pruned, newdata=test_temp, type="class")
length(cart_all_up_pruned_pred)
#confusion matrix
confusionMatrix(data=cart_all_up_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

### CART - NUT, BIN

```{r}
#using all features
train_temp <- upsamp_tr_bin %>% 
  select(all_of(nutritional_values), contains("bin"))

test_temp <- test_bin %>% 
    select(all_of(nutritional_values), contains("bin"))
```

#### Plotting initial tree

```{r}
cart_NuBi_up <- rpart(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_up)
```

#### Pruning the tree

We decided to use the package "adabag" to automatically prune the tree, using the 1-SE rule. The CP evaluation relies on a cross-validation procedure, which is why we need to set the seed before using the function.

```{r}
set.seed(12)
cart_NuBi_up_pruned <- autoprune(rating_bin ~ ., data = train_temp)

rpart.plot(cart_NuBi_up_pruned, roundint=FALSE)
```

#### Making predictions

Classifies everything as good.

```{r}
#predicting the test set
cart_NuBi_up_pruned_pred <- predict(cart_NuBi_up_pruned, newdata=test_temp, type="class")
length(cart_all_up_pruned_pred)
#confusion matrix
confusionMatrix(data=cart_NuBi_up_pruned_pred, reference = test_temp$rating_bin, positive="good")
```

## 2.1 All variables

## 3.1 PCA Variables
