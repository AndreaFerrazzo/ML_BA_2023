# Supervised Learning

## 0. Data Normalisation, splitting and balancing
Creating a new dataframe for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features. We name it `recipes_analysis`.

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))
  
###### CAREFUL --> recipes_analysis should be of dim 10163 x 33
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(across(all_of(contains("bin")), as.factor) , ID = as.character(ID)) %>% 
  select(rating, all_of(nutritional_values), contains("bin"), contains("total"))
```

Creating a normalized version of `recipes_analysis` where the continuous numerical values are normalised, to remain consistent across all models for our analysis.

```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

recipes_analysis_normd <- recipes_analysis %>% 
  mutate(rating = as.factor(rating), across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split

```{r}
#creating training and test sets
set.seed(12)
index <- createDataPartition(recipes_analysis_normd$rating, p=0.75, list = FALSE)
```

We create a training set with the original `rating` variable with 7 levels. 

Below we can see the multilevel rating training set is really unbalanced throughout the classes.
```{r}
#creating training and test set with multilevel rating variable
train_multi <- recipes_analysis_normd[index, ]
test_multi <- recipes_analysis_normd[-index, ]

table(train_multi$rating)
```


### Creating train and test sets for some variable combination
```{r}
#for train multi
tr_multi_all <- train_multi
te_multi_all <- test_multi

tr_multi_Nut <- train_multi %>% 
  select(rating, all_of(nutritional_values))
te_multi_Nut <- test_multi %>% 
  select(rating, all_of(nutritional_values))
```

### TrainControl Functions
Here we create the train control functions we will use throughout the project.
```{r trCtrl, cache=FALSE}
#creating train control data for unbalanced data
trCtrl <- trainControl(method = "cv",
                       number = 5)
```

```{r trCtrl_down, cache=FALSE}
#train control with downsampling
trCtrl_down <- trainControl(method = "cv",
                       number = 5,
                       sampling = "down")
```

```{r trCtrl_down_twoClass, cache = FALSE}
#train control with downsampling twoclass summary
trCtrl_down_twoClass <- trainControl(method = "cv",
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE,
                       number = 5,
                       sampling = "down"
                       )
```

## 1.0 Testing performance on 7-level rating classification with RF

Given our correlation results in EDA, we anticipated that a classification with 7 levels will basically be equivalent to randomly classifying each observation in one of the 7 rating levels. However, to make sure this is the case, we try running a random forest on the nutritional values, as well as both the "total" and "bin" ingredients-related features we created during the EDA.
We use a random forest for this testing phase because, being an ensemble method, it should give us reasonable results if such results are possible with the data we have. If the results are bad, we can assume that other models will not magically classify everything perfectly.

### Fitting RF - multi_all

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 45.6
- Kappa 0.002
```{r rf_multi_all_fit}
set.seed(12)
rf_multi_all_fit <- caret::train(rating ~ .,
                     data = tr_multi_all,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_all_fit
plot(rf_multi_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions RF - multi_all
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r rf_multi_all_pred}
#make predictions
rf_multi_all_pred <- predict(rf_multi_all_fit, newdata = test_multi)

#confusion matrix 
confusionMatrix(data = rf_multi_all_pred, reference = test_multi$rating)
```

### Fitting - RF multi_Nut
We now try another RF but only with the nutritional values as explanatory variables to see if it changes something.

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 41.6
- Kappa 0.048

```{r rf_multi_Nut_fit}
set.seed(12)
rf_multi_Nut_fit <- train(rating ~ .,
                     data = tr_multi_Nut,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl,
                     tuneLength = 3)
#show the random forest with cv 
rf_multi_Nut_fit
plot(rf_multi_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

### Predictions - RF multi_Nut

As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r rf_multi_Nut_pred}
#make predictions
rf_multi_Nut_pred <- predict(rf_multi_Nut_fit, newdata = te_multi_all)

#confusion matrix 
confusionMatrix(data = rf_multi_Nut_pred, reference = te_multi_Nut$rating)
```

### Evaluation and decision for rest of analysis
Since we see a huge bias towards the majority class in unbalanced data (which we expected), we have to take a decision between 3 options:

- We could try balancing the 7 levels, either up or down. Due to the very large disparity in observation counts per level (min at 32 and max at 3481), we believe than neither up- nor downsampling would be ideal. Indeed, upsampling would mean creating a massive amount of artificial observations through replacement for the minority classes, while downsampling would leave us with an insufficient total amount of observations.
- We could try to aggregate the minority classes into one single class (i.e., putting  1.25, 1.875, 2.5, and 3.125 together), leaving us with 4 final classes. There would still be a significant imbalance however, and given the near-random classification that we observed above, we do not believe aggregating those classes is the solution.
- We could transform the 7 levels into a binary classification problem. This would help both with class imbalance and would hopefully also lead to higher accuracy metrics by simplifying the classification task.

After careful reflection, we believe that the 3rd options is our best bet. We therefore transform the 7-level rating variable into a binary rating variable, with the threshold at 4. This means that all ratings below 4 are now considered as "bad" and all ratings above 4 are considered as "good".

## 1.1 33 Engineered variables - unbalanced

```{r summary_fun, cache=FALSE}
#function to create the summary tables from the confusion matrices
summary_table_fun <- function(x){
  acc <- x[[3]][[1]]
  kap <- x[[3]][[2]]
  bacc <- x[[4]][[11]]
  sens <- x[[4]][[1]]
  vec <- c(acc, bacc, sens, kap)
  return(vec)
}

#creating empty summary table for unbalanced data
summary_table_unbal <- tibble(metric = c("accuracy", "balanced_accuracy", "sensitivity", "kappa"))
```

### Data preparation for analysis

#### Creating new rating_bin variable
We transform the original 7-level rating variable into a binary `rating_bin` variable, "bad" or "good" (i.e., with a threshold at 4 to decide if the rating is bad or good).

```{r}
#creating binary rating variable
recipes_analysis_bin <- recipes_analysis %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  select(-rating)

#reversing rating_bin factor order
recipes_analysis_bin$rating_bin <- factor(recipes_analysis_bin$rating_bin, levels=rev(levels(recipes_analysis_bin$rating_bin )))

#normalising newly created df
recipes_analysis_bin <- recipes_analysis_bin %>% 
  mutate(across(where(is.numeric), my_normalise))
```

#### Creating rating_bin training and test sets
```{r}
#creating training and test sets
set.seed(12)
index_bin <- createDataPartition(recipes_analysis_bin$rating_bin, p=0.75, list = FALSE)
```

We create a training and test set with the new `rating_bin` variable with only 2 levels. 

Below we can see the binary rating training set is still slightly unbalanced throughout the two classes.
```{r}
#creating training and test set with multilevel rating variable
train_bin <- recipes_analysis_bin[index_bin, ]
test_bin <- recipes_analysis_bin[-index_bin, ]

table(train_bin$rating_bin)
```

#### Creating various train/test sets with different variable combinations
```{r}
#creating train and test set for all combinations of variables, for binary rating df
train_bin_Nut <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))
test_bin_Nut <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))

train_bin_NuTo <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
test_bin_NuTo <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

train_bin_NuBi <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_bin_NuBi <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))

train_bin_Tot <- train_bin %>% 
  select(rating_bin, all_of(contains("total")))
test_bin_Tot <- test_bin %>% 
  select(rating_bin, all_of(contains("total")))

train_bin_Bin <- train_bin %>% 
  select(rating_bin, all_of(contains("bin")))
test_bin_Bin <- test_bin %>% 
  select(rating_bin, all_of(contains("bin")))
```


### Logistic Regression - unbalanced - Nut

#### Fitting
We now fit a logreg model on unbalanced data with only the nutritional variables.

```{r logr_Nut_unbal_fit}
set.seed(12)
logr_Nut_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_Nut_unbal_fit
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying all observations as good, resulting in a perfect sensitivity but a 0 specificity and a kappa of 0.

```{r logr_Nut_unbal_pred}
logr_Nut_unbal_pred <- predict(logr_Nut_unbal_fit, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = logr_Nut_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_Nut = summary_table_fun(CM))
```

### Logistic Regression - unbalanced - NuBi

#### Fitting
We now fit a logreg model on unbalanced data with the NuBi variable combination.

```{r logr_NuBi_unbal_fit}
set.seed(12)
logr_NuBi_unbal_fit <- train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_NuBi_unbal_fit
```

#### Predictions

The accuracy here is slightly lower at 58.5%, however, the model now is a bit less biased towards the majority class and classifies at least some obs as bad. Kappa is still very low at 1.9%

```{r logr_NuBi_unbal_pred}
logr_NuBi_unbal_pred <- predict(logr_NuBi_unbal_fit, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = logr_NuBi_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_NuBi = summary_table_fun(CM))
```

### Logistic Regression - unbalanced - NuTo

#### Fitting
We now fit a logreg model on unbalanced data with the NuTo variable combination.
```{r logr_NuTo_unbal_fit}
set.seed(12)
logr_NuTo_unbal_fit <- train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "glm",
                family = "binomial",
                trControl = trCtrl)
logr_NuTo_unbal_fit
```

#### Predictions
Accuracy is slightly higher than with the NuBi combination, at 58.9%. The sensitivity is very similar and the kappa is slighty higher at 2.9%
```{r logr_NuTo_unbal_pred}
logr_NuTo_unbal_pred <- predict(logr_NuTo_unbal_fit, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = logr_NuTo_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, LogReg_NuTo = summary_table_fun(CM))
```
Out of the 3 LogReg models with unbalanced data, the NuTo model seems to perform the best.
### KNN - unbalanced - Nut

#### Fitting
We fit a KNN model on unbalanced data with only the nutritional variables. The best hyperparameter K if we look at both accuracy and Kappa is 113.

```{r knn_Nut_unbal_fit}
set.seed(12)
knn_Nut_unbal_fit <- caret::train(rating_bin ~.,
                                  data = train_bin_Nut,
                                  method = "knn",
                                  trControl = trCtrl,
                                  tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 113
                                  )
knn_Nut_unbal_fit
```

#### Predictions

We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r knn_Nut_unbal_pred}
knn_Nut_unbal_pred <- predict(knn_Nut_unbal_fit, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = knn_Nut_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_Nut = summary_table_fun(CM))
```

### KNN - unbalanced - NuBi

#### Fitting
Best K if we look at both accuracy and Kappa is 135

```{r knn_NuBi_unbal_fit}
set.seed(12)
knn_NuBi_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "knn",
                trControl = trCtrl,
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi_unbal_fit
```

#### Predictions

Once again most obs are classified as good. Not better than NIR.

```{r knn_NuBi_unbal_pred}
knn_NuBi_unbal_pred <- predict(knn_NuBi_unbal_fit, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = knn_NuBi_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_NuBi = summary_table_fun(CM))
```

### KNN - unbalanced - NuTo

#### Fitting
Best K if we look at both accuracy and Kappa is 135

```{r knn_NuTo_unbal_fit}
set.seed(12)
knn_NuTo_unbal_fit <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "knn",
                trControl = trCtrl,
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuTo_unbal_fit
```

#### Predictions

Once again most obs are classified as good. Not better than NIR.

```{r knn_NuTo_unbal_pred}
knn_NuTo_unbal_pred <- predict(knn_NuTo_unbal_fit, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = knn_NuTo_unbal_pred, positive="good")
CM

summary_table_unbal <- cbind(summary_table_unbal, KNN_NuTo = summary_table_fun(CM))
```


### CART - unbalanced -  Nut

#### Fitting
```{r cart_nut_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_Nut_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_Nut,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_Nut_unbal_fit
```


```{r cart_nut_unbal_results}
# Get the results for each CP step
cart_Nut_unbal_results <- cart_Nut_unbal_fit$results

cart_Nut_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nut_unbal_pred}
#Now using the cv to predict test set
cart_Nut_unbal_pred <- predict(cart_Nut_unbal_fit, newdata = test_bin_Nut)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_Nut_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_Nut = summary_table_fun(CM))
```

### CART - unbalanced - NuBi

#### Fitting
```{r cart_nubi_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuBi_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_NuBi,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_NuBi_unbal_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nubi_unbal_results}
# Get the results for each CP step
cart_NuBi_unbal_results <- cart_NuBi_unbal_fit$results

cart_NuBi_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nubi_unbal_pred}
#Now using the cv to predict test set
cart_NuBi_unbal_pred <- predict(cart_NuBi_unbal_fit, newdata = test_bin_NuBi)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = cart_NuBi_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_NuBi = summary_table_fun(CM))
```

### CART - unbalanced - NuTo

#### Fitting
```{r cart_NuTo_unbal_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuTo_unbal_fit <- train(rating_bin ~ .,
                  data = train_bin_NuTo,
                  method = "rpart",
                  trControl = trCtrl,
                  tuneLength = 10)
cart_NuTo_unbal_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_NuTo_unbal_results}
# Get the results for each CP step
cart_NuTo_unbal_results <- cart_NuTo_unbal_fit$results

cart_NuTo_unbal_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_NuTo_unbal_pred}
#Now using the cv to predict test set
cart_NuTo_unbal_pred <- predict(cart_NuTo_unbal_fit, newdata = test_bin_NuTo)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = cart_NuTo_unbal_pred)
CM

summary_table_unbal <- cbind(summary_table_unbal, CART_NuTo = summary_table_fun(CM))
```

## 1.2. 33 Engineered variables - balanced

```{r summary_table_down_create, cache=FALSE}
#creating empty summary table for balanced data
summary_table_down <- tibble(metric = c("accuracy", "balanced_accuracy", "sensitivity", "kappa"))
```

### Random Forest - balanced - Nut

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 53.8
- BACC 53.8
- Kappa 7.40
- Recall 53.9

#### Fitting
```{r rf_nut_fit}
set.seed(12)
rf_Nut_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Nut,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Nut_fit
plot(rf_Nut_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
As can be seen by the predictions below, our assumptions were valid. The model does a really bad job at classifying the test set into the correct rating categories. It is equivalent to a random model with a very strong bias towards the majority class at 4.375 (due to the data imbalance present in the `rating` variable.)
```{r rf_nut_pred}
set.seed(12)
#make predictions
rf_Nut_pred <- predict(rf_Nut_fit, newdata = test_bin_Nut)

#confusion matrix 
CM <- confusionMatrix(data = rf_Nut_pred, reference = test_bin_Nut$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_Nut = summary_table_fun(CM))
```

### Random Forest - balanced - NuBi

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- ACC 54.8
- BACC 54.2
- Kappa 8.20
- Recall 57.7

#### Fitting
```{r rf_nubi_fit}
set.seed(12)
rf_NuBi_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuBi,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuBi_fit
plot(rf_NuBi_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_nubi_pred}
set.seed(12)
#make predictions
rf_NuBi_pred <- predict(rf_NuBi_fit, newdata = test_bin_NuBi)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuBi_pred, reference = test_bin_NuBi$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuBi = summary_table_fun(CM))
```

### Random Forest - balanced - NuTo

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 56.4
- BACC 55.8
- Kappa 11.3
- Recall 59.3

#### Fitting
```{r rf_nuto_fit}
set.seed(12)
rf_NuTo_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_NuTo,
                     method = "rf",
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_NuTo_fit
plot(rf_NuTo_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_nuto_pred}
set.seed(12)
#make predictions
rf_NuTo_pred <- predict(rf_NuTo_fit, newdata = test_bin_NuTo)

#confusion matrix 
CM <- confusionMatrix(data = rf_NuTo_pred, reference = test_bin_NuTo$rating_bin)
CM

summary_table_down <- cbind(summary_table_down, RF_NuTo = summary_table_fun(CM))
```

### Random Forest - balanced - Tot

**Hyperparameters**

Best tune: mtry = 3

**Accuracy metrics**

- ACC 52.5
- BACC 52.4
- Kappa 4.6
- Recall 52.9

#### Fitting
```{r rf_tot_fit}
set.seed(12)
rf_Tot_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Tot,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Tot_fit
plot(rf_Tot_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_tot_pred}
set.seed(12)
#make predictions
rf_Tot_pred <- predict(rf_Tot_fit, newdata = test_bin_Tot)

#confusion matrix 
CM <- confusionMatrix(data = rf_Tot_pred, reference = test_bin_Tot$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Tot = summary_table_fun(CM))
```

### Random Forest - balanced - Bin

**Hyperparameters**

Best tune: mtry = 2

**Accuracy metrics**

- ACC 52.9
- BACC 52.4
- Kappa 4.6
- Sens 55.4

#### Fitting
```{r rf_bin_fit}
set.seed(12)
rf_Bin_fit <- caret::train(rating_bin ~ .,
                     data = train_bin_Bin,
                     method = "rf",
                     preProcess = NULL,
                     trControl = trCtrl_down,
                     tuneLength = 10)
#show the random forest with cv 
rf_Bin_fit
plot(rf_Bin_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_bin_pred}
set.seed(12)
#make predictions
rf_Bin_pred <- predict(rf_Bin_fit, newdata = test_bin_Bin)

#confusion matrix 
CM <- confusionMatrix(data = rf_Bin_pred, reference = test_bin_Bin$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_Bin = summary_table_fun(CM))
```

We see that the 3 best performing models are: Nut, NuBi and NuTo. Which was expected given the importance of the nutritional values that we saw in the PCA during the EDA. Therefore, we decide to test further models only with those 3 combinations of variables, thus leaving out models trained solely on Tot or on Bin.


### Logistic Regression - balanced - Nut

#### Fitting

```{r logr_Nut_blncd}
set.seed(12)
logr_Nut_blncd <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_Nut_blncd
```
We observe that the accuracy of training set is 49.6%.

#### Predictions

```{r logr_Nut_pred_blncd}
logr_Nut_pred_blncd <- predict(logr_Nut_blncd, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = logr_Nut_pred_blncd, positive="good")
CM

summary_table_down <- cbind(summary_table_down, LogReg_Nut = summary_table_fun(CM))
```
Here we notice that the accuracy of the test set stands at 47.9%


### Logistic Regression - balanced - NuBi

#### Fitting

```{r logr_NuBi_blncd}

set.seed(12)
logr_NuBi_blncd <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_NuBi_blncd
```
We observe that the accuracy of the training set stands at 53.9%.

#### Predictions

```{r logr_NuBi_pred_blncd}
logr_NuBi_pred_blncd <- predict(logr_NuBi_blncd, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = logr_NuBi_pred_blncd, positive="good")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, LogReg_NuBi = summary_table_fun(CM))
```
We want to evaluate the quality of the model and we notice that the accuracy of the test set stands at 54.6%


### Logistic Regression - balanced - NuTo

#### Fitting

```{r logr_NuTo_blncd}

set.seed(12)
logr_NuTo_blncd <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "glm",
                family = "binomial",
                trControl = trCtrl_down)
logr_NuTo_blncd
```
We observe that the accuracy of the training set stands at 54.1%.

#### Predictions

```{r logr_NuTo_pred_blncd}
logr_NuTo_pred_blncd <- predict(logr_NuTo_blncd, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = logr_NuTo_pred_blncd, positive="good")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, LogReg_NuTo = summary_table_fun(CM))
```
We want to evaluate the quality of the model and we notice that the accuracy of the test set stands at 56.1%


### SVM - balanced - Nut

#### Linear SVM - balanced - Nut

```{r svm_Linear_nutritional, eval=FALSE}

svm_Linear_nutritional <- train(rating_bin ~ calories + protein + fat + sodium, data=train_bin_Nut, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_nutritional

```

The validation accuracy stands at 42.8%, which is not satisfying considering that it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_nutritional, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_nutritional

plot(svm_Linear_Grid_nutritional)

svm_Linear_Grid_nutritional$bestTune

```

The result indicates that setting the cost to C=10 provides the best model with accuracy=42.9%. The accuracy apparently reaches a plateau at this value. There is no sensible improvement compared to the previous linear SVM with default parameter cost C=1.

#### Radial SVM - balanced - Nut

```{r svm_Radial_nutritional, eval=FALSE}

svm_Radial_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_nutritional

```
The validation accuracy stands at 54.8%, which is substantially better than the SVM model with the linear kernel. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_nutritional, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_nutritional

plot(svm_Radial_Grid_nutritional)

svm_Radial_Grid_nutritional$bestTune

```

The optimal model from this search is with sigma = 0.1 and C = 1000 This optimal model would then reach accuracy=52.6%.

#### SVM Nut - Radial Kernel Best model

```{r recipe_rb_tuned_nutritional}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.1, C = 1000)


recipe_rb_tuned_nutritional <- train(rating_bin ~., data=train_bin_Nut, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_nutritional}
recipe_rb_tuned_pred_nutritional <- predict(recipe_rb_tuned_nutritional, newdata = test_bin_Nut)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_nutritional, reference = test_bin_Nut$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_Nut = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52.9% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1000 and sigma=0.01 is the best model.

### SVM - balanced - NuBi

#### Linear SVM - balanced - NuBi

```{r svm_Linear_NuBi, eval=FALSE}

svm_Linear_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_NuBi

```

The validation accuracy stands at 56.1%. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_NuBi, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_NuBi

plot(svm_Linear_Grid_NuBi)

svm_Linear_Grid_NuBi$bestTune

```

The result indicates that setting the cost to C=0.01 provides the best model with accuracy=57.6%. There is a sensible improvement compared to the previous linear SVM with default parameter cost C=1.


#### Radial SVM - balanced - NuBi

```{r svm_Radial_NuBi, eval=FALSE}

svm_Radial_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_NuBi

```
The validation accuracy stands at 53.7%, which is worse than the SVM model with the linear kernel. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_NuBi, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_NuBi

plot(svm_Radial_Grid_NuBi)

svm_Radial_Grid_NuBi$bestTune

```

The optimal model from this search is with sigma = 0.01 and C = 1. This optimal model would then reach accuracy=54.8%.

#### SVM NuBi - Radial Kernel Best model

```{r recipe_rb_tuned_NuBi}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.01, C = 1)


recipe_rb_tuned_NuBi <- train(rating_bin ~., data=train_bin_NuBi, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_NuBi}
recipe_rb_tuned_pred_NuBi <- predict(recipe_rb_tuned_NuBi, newdata = test_bin_NuBi)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_NuBi, reference = test_bin_NuBi$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_NuBi = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 54.1% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1 and sigma=0.01 is the best model.


### SVM - balanced - NuTo

#### Linear SVM - balanced - NuTo

```{r svm_Linear_NuTo, eval=FALSE}

svm_Linear_NuTo <- train(rating_bin ~ calories + protein + fat + sodium, data=train_bin_NuTo, method = "svmLinear", trControl=trCtrl_down)
svm_Linear_NuTo

```

The validation accuracy stands at 42.8%, which is not satisfying considering that it is computed on the training set. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.

##### Tuning the Hyperparameters - Linear basis SVM
```{r svm_Linear_Grid_NuTo, eval=FALSE}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmLinear", trControl=trCtrl_down, tuneGrid = grid)
svm_Linear_Grid_NuTo

plot(svm_Linear_Grid_NuTo)

svm_Linear_Grid_NuTo$bestTune

```

The result indicates that setting the cost to C=1000 provides the best model with accuracy=54%. There is a considerable improvement with respect to the previous linear SVM with default parameter cost C=1.

#### Radial SVM - balanced - NuTo

```{r svm_Radial_NuTo, eval=FALSE}

svm_Radial_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down)
svm_Radial_NuTo

```
The validation accuracy stands at 53.6%, which is substantially better than the SVM model with the linear kernel and default parameters. The next step consists in creating a grid of values for the cost that we want to try and pass to the argument tuneGrid.


##### Tuning the Hyperparameters - Radial basis SVM

```{r svm_Radial_Grid_NuTo, eval=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial)
svm_Radial_Grid_NuTo

plot(svm_Radial_Grid_NuTo)

svm_Radial_Grid_NuTo$bestTune

```

The optimal model from this search is with sigma = 0.01 and C = 1 This optimal model would then reach accuracy=53.7% .


#### SVM NuTo - Radial Kernel Best model

```{r recipe_rb_tuned_NuTo}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

grid_radial_best <- expand.grid(sigma = 0.01, C = 1)


recipe_rb_tuned_NuTo <- train(rating_bin ~., data=train_bin_NuTo, method = "svmRadial", trControl=trCtrl_down, tuneGrid = grid_radial_best)
```


```{r recipe_rb_tuned_pred_NuTo}
recipe_rb_tuned_pred_NuTo <- predict(recipe_rb_tuned_NuTo, newdata = test_bin_NuTo)

CM <- confusionMatrix(data=recipe_rb_tuned_pred_NuTo, reference = test_bin_NuTo$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, SVM_Radial_NuTo = summary_table_fun(CM))

```

The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 55.4% on the test set. We can conclude that among all the models, the radial basis kernel SVM with C=1 and sigma=0.01 is the best model.

### KNN - balanced - Nut

#### Fitting

```{r knn_nut_fit}
set.seed(12)
knn_Nut <- caret::train(rating_bin ~.,
                data = train_bin_Nut,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))#initially checked on 1-151 range, best was 111
                )
knn_Nut
```

#### Predictions

```{r knn_nut_pred}
knn_Nut_pred <- predict(knn_Nut, newdata = test_bin_Nut, type = "raw")

CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = knn_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, KNN_Nut = summary_table_fun(CM))
```

### KNN - balanced - NuBi

#### Fitting

```{r knn_nubi_fit}
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_bin_NuBi,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))#initially checked on 1-151 range, best was 37
                )
knn_NuBi
```

#### Predictions

```{r knn_nubi_pred}
knn_NuBi_pred <- predict(knn_NuBi, newdata = test_bin_NuBi, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuBi$rating_bin, data = knn_NuBi_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuBi = summary_table_fun(CM))
```

### KNN - balanced - NuTo

#### Fitting

```{r knn_nuto_fit}
set.seed(12)
knn_NuTo <- caret::train(rating_bin ~.,
                data = train_bin_NuTo,
                method = "knn",
                trControl = trCtrl_down,
                tuneGrid= data.frame(k = seq(1, 150,by = 5))
                )
knn_NuTo
```

#### Predictions

```{r knn_nuto_pred}
knn_NuTo_pred <- predict(knn_NuTo, newdata = test_bin_NuTo, type = "raw")

CM <- confusionMatrix(reference = test_bin_NuTo$rating_bin, data = knn_NuTo_pred, positive="good")
CM

summary_table_down <- cbind(summary_table_down, KNN_NuTo = summary_table_fun(CM))
```

### CART - balanced -  Nut

**Hyperparameters**

Best parameter CP is 0.0035

**Accuracy Metrics**

- ACC 54.80

#### Fitting
```{r cart_nut_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_Nut_fit <- train(rating_bin ~ .,
                  data = train_bin_Nut,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_Nut_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nut_results}
# Get the results for each CP step
cart_Nut_results <- cart_Nut_fit$results

cart_Nut_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nut_pred}
#Now using the cv to predict test set
cart_Nut_pred <- predict(cart_Nut_fit, newdata = test_bin_Nut)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_Nut_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_Nut = summary_table_fun(CM))
```

### CART - balanced - NuBi

**Hyperparameters**

Best parameter CP is 0.00546

**Accuracy Metrics**

- ACC 53.7

#### Fitting
```{r cart_nubi_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuBi_fit <- train(rating_bin ~ .,
                  data = train_bin_NuBi,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuBi_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nubi_results}
# Get the results for each CP step
cart_NuBi_results <- cart_NuBi_fit$results

cart_NuBi_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nubi_pred}
#Now using the cv to predict test set
cart_NuBi_pred <- predict(cart_NuBi_fit, newdata = test_bin_NuBi)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuBi_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuBi = summary_table_fun(CM))
```

### CART - balanced - NuTo

**Hyperparameters**

Best parameter CP is 0.00257

**Accuracy Metrics**

- ACC 53.7

#### Fitting
```{r cart_nuto_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_NuTo_fit <- train(rating_bin ~ .,
                  data = train_bin_NuTo,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_NuTo_fit
```

It might seem like the CP chosen by caret is not optimal since the accuracy curve continues to go up, however, we confirmed by using a manual tuning grid that this CP was indeed the best when combining the accuracy with the Kappa metric.
```{r cart_nuto_results}
# Get the results for each CP step
cart_NuTo_results <- cart_NuTo_fit$results

cart_NuTo_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_nuto_pred}
#Now using the cv to predict test set
cart_NuTo_pred <- predict(cart_NuTo_fit, newdata = test_bin_NuTo)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_NuTo_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_NuTo = summary_table_fun(CM))
```
### XGBoost - balanced - Nut

## 2.1 All variables (Nut + 293 ingredients)

### Creating dataset
```{r}
analysis_all <- recipes %>% 
  select(rating, all_of(c(nutritional_values, all_ingredients))) %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good")), across(all_of(all_ingredients), as.factor)) %>% 
  select(-rating) %>% 
  select(rating_bin, everything())

#reversing rating_bin factor order
analysis_all$rating_bin <- factor(analysis_all$rating_bin, levels=rev(levels(analysis_all$rating_bin)))

#normalising newly created df
analysis_all <- analysis_all %>% 
  mutate(across(where(is.numeric), my_normalise))
```

```{r}
#we noticed that 5 ingredients appear in none of the recipes
ingredients_to_remove <- analysis_all %>%
  select_if(~nlevels(.) == 1) %>% colnames()

#removing those 5 ingredients
analysis_all <- analysis_all %>% 
  select(-all_of(ingredients_to_remove))
```

```{r}
#creating training and test sets
set.seed(12)
index_all <- createDataPartition(analysis_all$rating_bin, p=0.75, list = FALSE)
```

We create a training set with the new `rating_bin` variable with only 2 levels, including the nutritional and all ingredients dummies, instead of the engineered variables used in the sub-section above.

```{r}
#creating training and test set with multilevel rating variable
train_all <- analysis_all[index_all, ]
test_all <- analysis_all[-index_all, ]

table(train_all$rating_bin)
```

#### Balancing the binary training set through manual downsampling

```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_all %>%  filter(rating_bin == "good")
tr_bad <- train_all %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_good <- sample(x = 1:nrow(tr_good), size = nrow(tr_bad), replace = FALSE)
downsamp_tr_all <- tibble(rbind(tr_good[index_good,], tr_bad))

#checking that we have the correct number of good and bad
table(downsamp_tr_all$rating_bin)
```

```{r}
#manual upsampling

# #filtering by rating class
# set.seed(12)
# tr_good <- train_bin %>%  filter(rating_bin == "good")
# tr_bad <- train_bin %>%  filter(rating_bin == "bad")
# 
# #indexing "bad" and creating new resampled training set
# index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
# upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))
# 
# #checking that we have the correct number of good and bad
# table(upsamp_tr_bin$rating_bin)
```

### Random Forest - balanced - All Var

**Hyperparameters**

Best tune: mtry = 292

**Accuracy metrics**
With tunelength = 2
- ACC 56.7
- BACC 56.8
- Kappa 13.1
- Recall 56.2

#### Fitting
```{r rf_all_fit}
set.seed(12)
rf_all_fit <- train(rating_bin ~ .,
                    data = train_all,
                    method = "rf",
                    trControl = trCtrl_down,
                    tuneLength = 3)
#show the random forest with cv 
rf_all_fit
plot(rf_all_fit, main = "Plot of CV Accuracy relative to Mtry parameter")
```

#### Predictions
```{r rf_all_pred}
set.seed(12)
#make predictions
rf_all_pred <- predict(rf_all_fit, newdata = test_all)

#confusion matrix 
CM <- confusionMatrix(data = rf_all_pred, reference = test_all$rating_bin)
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, RF_all = summary_table_fun(CM))
```

### Logistic Regression

```{r logreg_all_fit}
#too long
logreg_all_fit <- train(rating_bin~., 
                       data=downsamp_tr_all, 
                       method="glm", 
                       family = "binomial",
                       trControl=trCtrl_down,
                       trace = FALSE)
logreg_all_fit
```

### Predictions
```{r logreg_all_pred}
#predicting
logreg_all_pred <- predict(logreg_all_fit, newdata = test_all)
#creating confusion matrix
CM <- confusionMatrix(reference = test_all$rating_bin, data = logreg_all_pred)
CM
#adding results to the summary_table_unbal
summary_table_down <- cbind(summary_table_down, LogReg_all = summary_table_fun(CM))
```

### CART

**Hyperparameters**

Best parameter CP is 0.0035

**Accuracy Metrics**

- ACC 54.80

#### Fitting
```{r cart_all_fit}
#CART_grid <-  expand.grid(cp = c(0.001,0.005,0.01, 0.015, 0.02, 0.05, 0.1, 0.15, 0.2)) --> confirmed that highest kappa is at cp selected by caret
set.seed(12)
cart_all_fit <- train(rating_bin ~ .,
                  data = train_all,
                  method = "rpart",
                  trControl = trCtrl_down,
                  tuneLength = 10)
cart_all_fit
```

```{r cart_all_results}
# Get the results for each CP step
cart_all_results <- cart_all_fit$results

cart_all_results %>% 
  ggplot(aes(x=cp, y=Accuracy)) +
  geom_line()+
  labs(x = "CP Parameter", y = "Accuracy", title = "CP Plot")
```

#### Predictions
```{r cart_all_pred}
#Now using the cv to predict test set
cart_all_pred <- predict(cart_all_fit, newdata = test_all)
#And looking at the confusion matrix for the predictions using the cv 
CM <- confusionMatrix(reference = test_bin_Nut$rating_bin, data = cart_all_pred)
CM

summary_table_down <- cbind(summary_table_down, CART_all = summary_table_fun(CM))
```

### XGBoost
Here we decided to try the XGBoost model for the first time to see if it would magically work better than the others we have tested so far with other variable combinations.

```{r xgb_all datasets, cache=FALSE}
#creating datasets for xgboost
numeric_downsamp_tr <- downsamp_tr_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

numeric_te <- test_all %>% 
  mutate(rating_bin = ifelse(rating_bin == "good", 1, 0), across(everything(), as.character)) %>% 
  mutate(across(everything(), as.numeric))

#creating datasets which don't include the labels, for inputs in the xgb Matrix
explan_downsamp_tr <- numeric_downsamp_tr %>% 
  select(-rating_bin)

explan_te <- numeric_te %>% 
  select(-rating_bin)

#prepare data for XGBoost by creating xgb matrix
xgb_downsamp_tr <- xgb.DMatrix(data = as.matrix(explan_downsamp_tr), label = numeric_downsamp_tr$rating_bin)

xgb_te <- xgb.DMatrix(data = as.matrix(explan_te), label = numeric_te$rating_bin)
```


#### Fitting

**Accuracy metrics**

- ACC 59.4
- BACC 55.7
- Kappa 11.9
- Recall 76.4
```{r xgb_all_down_fit, cache=FALSE}
#set hyperparameters
param_list <- list(
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 7,
  max_depth = 5,
  min_child_weight = 1,
  subsample = 0.6,
  colsample_bytree = 0.6
)

#train the model
set.seed(12)
xgb_down_fit <- xgboost(data = xgb_downsamp_tr,
                     params = param_list,
                     nrounds = 100,
                     verbose = FALSE)
```

Here we control for overfitting by checking the balanced accuracy on the training set, which stands at a 61.3%. We have found a great balance between performance and overfitting by setting the gamma parameter at 7 and the nrounds at 100.
```{r xgb_down_prob_train}
#testing on training set to control overfitting
xgb_down_prob_train <- predict(xgb_down_fit, newdata = xgb_downsamp_tr)

xgb_down_pred_train <- as.factor(ifelse(xgb_down_prob_train<0.5, 0, 1))

CM <- confusionMatrix(reference = as.factor(numeric_downsamp_tr$rating_bin), data = xgb_down_pred_train, positive = "1")
CM
```

#### Predictions
We now check the actual balanced accuracy on the testing set. We see that the accuracy is at 56.0%. While this is slightly lower than what we found on the training set, it does not signal significant overfitting.
```{r xgb_all_down_pred, cache=FALSE}
xgb_down_prob <- predict(xgb_down_fit, newdata = xgb_te)

xgb_down_pred <- as.factor(ifelse(xgb_down_prob<0.5, 0, 1))

CM <- confusionMatrix(reference = as.factor(numeric_te$rating_bin), data = xgb_down_pred, positive = "1")
CM
#adding results to summary table
summary_table_down <- cbind(summary_table_down, XGB_all = summary_table_fun(CM))
```

## 3.1 Model Selection
We decide to select models only withing the models that we fitted with the binary rating variable. This leaves us with both the balanced and unbalanced sections.

### Unbalanced data
Here we can see the results for section 5.3 on the unbalanced data. We observe that the balanced accuracy is rarely too far from 50% which indicates a pretty bad classification. The sensitivity is also very high for most models due to the bias towards the majority class "good" due to the class imbalance. Given this poor performance, we decide to focus on the models with balanced data to perform our selection of the best model.
```{r summary_table_unbal, cache=FALSE}
summary_table_unbal %>% 
  kbl() %>% 
  kable_styling(position = "center")
summary_table_unbal
```

### Balanced data
Here we can see the results for section 5.4 on the downsampled data, with a total of 21 models that we fitted with various combinations of explanatory variables. We see that the balanced accuracy is already higher, and the sensitivity is more reasonable. Below, we investigate various aspects of these metrics and select the top-performing models. We decide to use balanced accuracy and sensitivity as the two core metrics to quantitatively rank the best models.
```{r summary_table_down, cache=FALSE}
summary_table_down %>% 
  kbl() %>% 
  kable_styling(position = "center")

summary_table_down_store <- summary_table_down
```

```{r}
#creating new df for model selection analysis plots
summary_down_long <- summary_table_down %>% 
  pivot_longer(-metric, names_to = "model", values_to = "value") %>% 
  select(model, everything()) %>% 
  mutate(value = value*100) %>% 
  arrange(model)

summary_down_wide <- summary_down_long %>% 
  pivot_wider(names_from = "metric", values_from = "value")

#selecting 3 models with highest balance accuracy
bacc_select <- summary_down_wide %>% 
  arrange(desc(balanced_accuracy)) %>% 
  dplyr::slice(1:5) %>% pull(model)
#selecting 3 models with highest sens
sens_select <- summary_down_wide %>% 
  arrange(desc(sensitivity)) %>% 
  dplyr::slice(1:5) %>% pull(model)
#selecting 3 models with highest kappa
kappa_select <- summary_down_wide %>% 
  arrange(desc(kappa)) %>% 
  dplyr::slice(1:5) %>% pull(model)
```

We first look at the top-5 models with the highest balanced_accuracy
```{r}
mean_bacc <- summary_down_wide %>% 
  summarise(mean_bacc = mean(balanced_accuracy)) %>% pull() %>% round(., 2)
            
summary_down_long %>% 
  filter(model %in% bacc_select, metric=="balanced_accuracy") %>% 
  ggplot(aes(x=model, y=value, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(50, 60)) + 
  labs(y = "Balance accuracy (in %)", x = "Model", title = "Top 5 models with highest balanced accuracy")
```

We now plot the top-5 models with the highest sensitivity. 
```{r}
mean_sens<- summary_down_wide %>% 
  summarise(mean_sens = mean(sensitivity)) %>% pull() %>% round(., 2)

summary_down_long %>% 
  filter(model %in% sens_select, metric=="sensitivity") %>% 
  ggplot(aes(x=model, y=value, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(55, 65)) + 
  labs(y = "Sensitivity (in %)", x = "Model", title = "Top 5 models with highest sensitivity")
```


```{r}
summary_points <- summary_down_wide %>% 
  mutate(rank_bacc = 22-rank(balanced_accuracy), points_bacc = rank(balanced_accuracy), rank_sens = 22-rank(sensitivity), points_sens = rank(sensitivity), total_points = (points_bacc + points_sens))

summary_points %>% 
  arrange(desc(total_points)) %>%
  dplyr::slice(1:5) %>% 
  select(model, points_bacc, points_sens, total_points) %>% 
  kbl() %>% 
  kable_styling(position = "center")
  
summary_points %>% 
  arrange(desc(total_points)) %>% 
  dplyr::slice(1:5) %>%
  ggplot(aes(x=model, y=total_points, fill = model))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(30, 40)) + 
  scale_y_continuous(breaks = seq(30, 40, by = 1))+
  labs(y = "Total number of points", x = "Model", title = "Top 5 models with highest number of points")
```
Here we can see that XGB_all has the highest number of points based on the weighted ranking system combining the balanced accuracy and sensitivity metrics with a total of 40 points. Followed by the RF_NuTo and LogReg_NuTo sharing the second place with 36 points.

### Showcasing the best model - XGBoost_all
Here we show 4 metrics for the best model we select. We can see that the accuracy and the balanced accuracy are very close to each other around 56%. The sensitivity is the highest at around 59.5%. Kappa is at 11.85% which is pretty low but still in the top values compared to other models we compared.
```{r}
summary_down_long %>% 
  filter(model =="XGB_all") %>% 
  ggplot(aes(x=model, y=value, fill = metric))+
  geom_bar(stat="identity", position = "dodge")+
  coord_cartesian(ylim = c(10, 60)) + 
  labs(y = "Value (in %)", x = "", title = "Showcasing 4 metrics for the XGB_all model")+
  scale_fill_viridis(discrete = TRUE)
```
#### Prediction robustness and plotting ROC curve of XGBoost_all
We can clearly see even the best model has a lot of trouble clearly classifying the results by attributing probabilities to each class. There is some severe overlap in probabilites around the threshold at 0.5. The median probability for each class is barely above and below the threshold for "good" and "bad" respectively. This indicates very low robustness of the model and confirms the poor classification performance.

```{r}
#predicting on the testing set
roc_df <- test_all %>% 
  mutate(prob_test = predict(xgb_down_fit, newdata = xgb_te))

#plotting histogram
roc_df %>%
  ggplot(aes(x = prob_test, fill = rating_bin)) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities on the test set - XGB_all")

#plotting boxplot
roc_df %>%
  ggplot(aes(x = rating_bin, y = prob_test)) +
  geom_boxplot() + 
  labs(x = "Response", y = "Probability", title = "Boxplot of predicted probabilities on the test - XGB_all")

```
Here we can see the ROC curve relating to the test set probabilities of the XGB_all model. This shows us that is is better than the random model, but not great peformance either as the AUC is clearly quite small.
```{r}
rocit_emp <- rocit(score = xgb_down_prob, class = numeric_te$rating_bin, method = "emp")

plot(rocit_emp, col = c(1,"gray50"), legend = TRUE, YIndex = FALSE)

```
## 3.2 Variable Importance and PDP

### VarImp
Below we show the first 50 most important variables from the top model we selected, XGB_all.
```{r VarImp XGBoost, fig.height=10, fig.width=8}
#XGBoost variable importance
xgb_importance_matrix <- xgb.importance(model = xgb_down_fit)
xgb_importance_matrix

xgb.ggplot.importance(importance_matrix = xgb_importance_matrix) +
  labs(title = "XGBoost Feature Importance")
```

### PDP
Doesn't work for xgboost
```{r eval = FALSE}
partial(xgb_down_fit, pred.var = "sodium", plot = TRUE)
```

### LIME
```{r eval=FALSE}
lime_xgb_explainer <- lime((explan_downsamp_tr), xgb_down_fit, bin_continuous = TRUE, quantile_bins = FALSE)
expl_xgb <- lime::explain(explan_te %>% select(sodium), lime_xgb_explainer, n_labels = 1, n_features = 1)
```

