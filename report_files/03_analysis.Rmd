---
title: "Analysis"
output: html_document
date: "2023-04-20"
---

```{r echo=FALSE, message=FALSE}
source(here::here("scripts/setup.R"))
```

# Supervised Learning

Creating a new df for analysis purposes, which includes all nutrition-related features, as well as all ingredient-related features
```{r}
my_normalise <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

```{r}
nutritional_df <- recipes %>% 
  select(ID, all_of(nutritional_values))

###### CAREFUL --> recipes_analysis should be of dim 10163 x 34
recipes_analysis <- ingredients_df_full %>% 
  left_join(nutritional_df, by="ID") %>% 
  mutate(rating_bin = as.factor(ifelse(rating<4, "bad", "good"))) %>% 
  mutate(across(all_of(contains("bin")), as.factor)) %>% 
  mutate(ID = as.character(ID)) %>% 
  select(ID, rating_bin, all_of(nutritional_values), contains("bin"), contains("total"))
```

Normalising the continuous numerical values, to remain consistent across all models.
```{r}
recipes_analysis <- recipes_analysis %>% 
  mutate(across(where(is.numeric), my_normalise))
```

Creating training and test set. We chose a 75/25 split
```{r}
set.seed(12)
index <- createDataPartition(recipes_analysis$rating_bin, p=0.75, list = FALSE)#data partition attemps to already balance out the data based on the outcome --> but here doesn't manage fully

train_bin <- recipes_analysis[index, ]
test_bin <- recipes_analysis[-index, ]
```

However, we can see that the data is pretty unbalanced between the 2 rating classes in the training set.
```{r}
table(train_bin$rating_bin)
```

Balancing the training set through upsampling
```{r}
#filtering by rating class
set.seed(12)
tr_good <- train_bin %>%  filter(rating_bin == "good")
tr_bad <- train_bin %>%  filter(rating_bin == "bad")

#indexing "bad" and creating new resampled training set
index_bad <- sample(x = 1:nrow(tr_bad), size = nrow(tr_good), replace = TRUE)
upsamp_tr_bin <- tibble(rbind(tr_good, tr_bad[index_bad,]))

#checking that we have the correct number of good and bad
table(upsamp_tr_bin$rating_bin)
```
# Analysis Unbalanced outcome variable

## Logistic Regression

```{r}

# to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# unbalanced data

train_bin$rating_bin <- ifelse(train_bin$rating_bin=="good",1,0)
test_bin$rating_bin <- ifelse(test_bin$rating_bin=="good",1,0)

```

```{r}

# since the data has already been splitted we proceed with the fitting of the logistic regression
# nutritional values and total ingredients

rating_logr <- glm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=train_bin, family="binomial")

summary(rating_logr)

```

### Variable selection and interpretation
```{r}

rating_logr_sel <- step(rating_logr)
summary(rating_logr_sel)

```
The variables total_ingredients and total_vegetables are statistically significant at alpha=0.01. Therefore, we can observe that the probability of having a "good" rating increases with a higher number of ingredients per recipe, but it decreases with a higher number of vegetables per recipe.

The variables fat and sodium are statistically significant at alpha=0.05. In this case we see that the probability of having a "good" rating increases with the content of fat and sodium per recipe.

### Inference
```{r}

prob_te_rating <- predict(rating_logr_sel, newdata = test_bin, type="response")
pred_te_rating <- ifelse(prob_te_rating >= 0.5, 1, 0)

table(Pred=pred_te_rating, Obs=test_bin$rating_bin)


```
As we can see the predictions are not satisfying. Since the number of good recipes is larger than the number of bad recipes, predicting a 1 always provides a good model overall. Anyway the model is not reliable when it comes to predict recipes with a 0. 

We believe that it is worth to proceed with the same analysis using balanced data.

## Logistic Regression with balanced data

```{r}

# to facilitate the use of logistic regression, we want to have a 0/1 outcome rather than a categorical one
# the same procedure on the test_bin has already been applied before

upsamp_tr_bin$rating_bin <- ifelse(upsamp_tr_bin$rating_bin=="good",1,0)

```

```{r}

# since the data has already been splitted we proceed with the fitting of the logistic regression
# nutritional values and total ingredients

rating_logr_up <- glm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, family="binomial")

summary(rating_logr_up)

```

### Variable selection and interpretation
```{r}

rating_logr_sel_up <- step(rating_logr_up)
summary(rating_logr_sel_up)

```
All the variables selected (total_ingredients, total_vegetables, fat and sodium) are statistically significant at alpha=0.01. Therefore, we can observe that the probability of having a "good" rating increases with a higher number of ingredients per recipe, but it decreases with a higher number of vegetables per recipe. Additionally, the probability of having a "good" rating increases with the content of fat and sodium per recipe.

### Inference

```{r}

prob_te_rating_up <- predict(rating_logr_sel_up, newdata = test_bin, type="response")
pred_te_rating_up <- ifelse(prob_te_rating_up >= 0.5, 1, 0)

table(Pred=pred_te_rating_up, Obs=test_bin$rating_bin)

```
Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has worsened at predicting "good" ratings. The use of balanced data has then proved to be useful.

## SVM

```{r}
library(e1071)
# with the svm() function of the e1071 package we can fit SVM to the data with several possible kernels 
# here we start with the linear kernel
# unbalanced data

train_bin$rating_bin <- as.factor(train_bin$rating_bin)
test_bin$rating_bin <- as.factor(test_bin$rating_bin)


recipe_svm <- svm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=train_bin, kernel="linear") # we fit the linear kernel
recipe_svm

```

Let us make predictions and check the accuracy
```{r}

recipe_svm_pred <- predict(recipe_svm, newdata = test_bin)

table(Pred=recipe_svm_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_pred, reference = test_bin$rating_bin)

```
In this case the accuracy stands at 59.2%, but we observe that the model is poorly predicting "bad" ratings. Before going too much further with the analysis of SVM, we will try to use balanced data to improve the accuracy.


## SVM - balanced data

```{r}

upsamp_tr_bin$rating_bin <- as.factor(upsamp_tr_bin$rating_bin)

recipe_svm_up <- svm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, kernel="linear") # we fit the linear kernel
recipe_svm_up

```

```{r}

recipe_svm_up_pred <- predict(recipe_svm_up, newdata = test_bin)

table(Pred=recipe_svm_up_pred, obs=test_bin$rating_bin) # we check predictions on the test set

# with the function confusionMatrix() of the library caret we get the accuracy measure which shows the proportion of correct predictions
confusionMatrix(data=recipe_svm_up_pred, reference = test_bin$rating_bin)

```
Compared to unbalanced data, we can observe that the model is now better at predicting "bad" ratings, even though it is has considerably worsened at predicting "good" ratings. This explains why with balanced data the accuracy stands at 50.4%.
We will proceed the rest of the analysis with balanced data.

### Radial basis SVM
```{r}

recipe_rb <- svm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, kernel="radial") # we fit the radial basis kernel
recipe_rb

```

Let us make predictions and check the accuracy
```{r}

recipe_rb_pred <- predict(recipe_rb, newdata = test_bin)
confusionMatrix(data=recipe_rb_pred, reference = test_bin$rating_bin)

```
The accuracy is now 51.6% meaning that the radial basis kernel seems to do slightly better than the linear one. However, we should not forget that we used default parameters so far. Let us try to do better by tuning hyperparameters.


### Tuning the Hyperparameters - Linear SVM
```{r}

trctrl <- trainControl(method = "cv", number=10)

svm_Linear <- train(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl)
svm_Linear

```
The validation accuracy stands at 52.9%, which is not that high even though it is computed on the training set.
The next step consists in creating a grid of values for the cost that we want to try and pass to the argugment tuneGrid.

```{r}

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

svm_Linear_Grid <- train(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, method = "svmLinear", trControl=trctrl, tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

```


```{r}

svm_Linear_Grid$bestTune

```
The result indicates that setting the cost to C=0.01 provides the best model with accuracy=53.2%. The accuracy apparently reaches a plateau at this value. It represents an improvement compared to the previous linear SVM with default parameter cost C=1.

### Tuning the Hyperparameters - Radial basis SVM
```{r}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial

svm_Radial_Grid <- train(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, method = "svmRadial", trControl=trctrl, tuneGrid = grid_radial)
svm_Radial_Grid

plot(svm_Radial_Grid)

```


```{r}

svm_Radial_Grid$bestTune

```
The optimal model from this search is with sigma = 0.1 and C = 1000
This optimal model would then reach accuracy=58.2%.

### Best model selection

```{r}
# After finding the best hyperparameters, we re-train the model with the best hyperparameters on the entire training set. Afterwards we will evaluate the model on the test set.

recipe_rb_tuned <- svm(rating_bin ~ calories + protein + fat + sodium + total_ingredients + total_meat + total_vegetables, data=upsamp_tr_bin, 
                       kernel = "radial", gamma = svm_Radial_Grid$bestTune$sigma, 
                       cost = svm_Radial_Grid$bestTune$C)

recipe_rb_tuned_pred <- predict(recipe_rb_tuned, newdata = test_bin)

confusionMatrix(data=recipe_rb_tuned_pred, reference = test_bin$rating_bin)

```
The result indicates that with the tuned hyperparameters on the radial basis SVM model we achieve an accuracy of 52% on the test set. We can conclude that among all the models, the radial basis kernel SVM with cost of 1000 and sigma equal to 0.1 is the best model.

## KNN

## KNN - NUT, BIN and TOTAL

```{r}
#using unbalanced training set here as apparently it's not required for KNN for data to be balanced
train_temp <- train_bin %>% 
  select(-ID)

test_temp <- test_bin %>% 
  select(-ID)
```

### Basic Model
Trying to fit a basic KNN model with arbitrary K value of 3. Results are not different than that of a No Information Rate (NIR) model.
```{r}
mod_knn <- knn3(data = train_temp, rating_bin ~., k=3)

knn_pred <- predict(mod_knn, newdata = test_temp, type = "class")

confusionMatrix(reference = test_temp$rating_bin, data = knn_pred, positive="good")
```

### Tuning K
Best K if we look at both accuracy and Kappa is 115.
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_all <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(111, 121,by = 2))#initially checked on 1-151 range, best was 115
                )
knn_all
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_all, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 115
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_all_pred <- predict(knn_all, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_all_pred, positive="good")
```

## KNN - only NUT

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 111.
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_Nu <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(105, 115,by = 2))#initially checked on 1-151 range, best was 111
                )
knn_Nu
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_Nu, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 111
We see that the relatively high accuracy is achieved by classifying most obs as good, resulting in a high sensitivity but a very poor specificity.

```{r}
knn_Nu_pred <- predict(knn_Nu, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Nu_pred, positive="good")
```

## KNN - NUT and TOTAL
```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))

test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("total")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 139
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuTo <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(135, 145,by = 2))#initially checked on 1-151 range, best was 139
                )
knn_NuTo
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuTo, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 139
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_NuTo_pred <- predict(knn_NuTo, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_NuTo_pred, positive="good")
```

## KNN - NUT and BIN

```{r}
train_temp <- train_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
test_temp <- test_bin %>% 
  select(rating_bin, all_of(nutritional_values), all_of(contains("bin")))
```

### Tuning K
Best K if we look at both accuracy and Kappa is 135
```{r}
#tuning k, using upsampled data
trCtrl <- trainControl(method = "cv",
                       #summaryFunction = defaultSummary,
                       #classProbs = TRUE,
                       number = 10
                       )
set.seed(12)
knn_NuBi <- caret::train(rating_bin ~.,
                data = train_temp,
                method = "knn",
                trControl = trCtrl,
                metric = "Accuracy",
                tuneGrid= data.frame(k = seq(131, 141,by = 2))#initially checked on 1-151 range, best was 135
                )
knn_NuBi
```

#### Roc curve
We can see that there is no clear separation at all between the probabilities of the good and bad rating. This is the sign of a classification that doesn't work.
```{r}
#predicting on the training set
df <- train_temp %>% 
  mutate(predicted_prob = predict(knn_NuBi, type = "prob")$"good")

df %>%
  ggplot() +
  aes(x = predicted_prob, fill = rating_bin) +
  geom_histogram(bins = 20) +
  labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities")
```

### Fitting cv model with K = 135
Once again most obs are classified as good. Not better than NIR.
```{r}
knn_Bi_pred <- predict(knn_NuBi, newdata = test_temp, type = "raw")

confusionMatrix(reference = test_temp$rating_bin, data = knn_Bi_pred, positive="good")
```

# Analysis - Balanced outcome variable

# Unsupervised Learning


